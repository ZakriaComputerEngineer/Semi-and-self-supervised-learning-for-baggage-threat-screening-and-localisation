{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# -*- coding: utf-8 -*-\n",
    "-----------------------------------------------------------------------------------\n",
    "# Author: Zakria Mehmood\n",
    "# DoC: 2025.04.15\n",
    "# email: zakriamehmood2001@gmail.com\n",
    "-----------------------------------------------------------------------------------\n",
    "# Description: COMPLETE NOTEBOOK OF THE CODE FOR EASE OF EXECUTION\n",
    "# This code is a PyTorch implementation of a Vision Transformer (ViT) Auto Encoder for a research article related to Semi & Self supervised learning.\n",
    "\n",
    "# The code includes the following components:\n",
    "# 1. Imports: Necessary libraries and modules for the implementation.\n",
    "# 2. Configuration: A configuration file that contains hyperparameters and settings for the model.\n",
    "# 3. Data Preparation: Functions to load and preprocess the dataset.\n",
    "# 4: All Utility functions: Functions for various utility tasks such as logging, saving checkpoints, and loading data.\n",
    "# 5. Model Definition: The Vision Transformer model architecture.\n",
    "# 6. Training and Evaluation: Functions to train the model and evaluate its performance.\n",
    "# 7. Testing: Functions to test the model on a specific dataset.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# PyTorch and related imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "import torch.utils.data.distributed\n",
    "import torch.multiprocessing as mp\n",
    "import torch.distributed as dist\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "\n",
    "# Vision and image processing\n",
    "import torchvision\n",
    "import torchvision.models as models\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "# Data manipulation and utilities\n",
    "import numpy as np\n",
    "from einops import rearrange, einops\n",
    "from tqdm import tqdm\n",
    "from easydict import EasyDict as edict\n",
    "from torchinfo import summary\n",
    "\n",
    "# System and miscellaneous\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import math\n",
    "import glob\n",
    "import copy\n",
    "import random\n",
    "import logging\n",
    "import warnings\n",
    "import yaml\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your model configurations dictionary\n",
    "MODEL_CONFIGS = {\n",
    "    'model_base': {\n",
    "        # Base model configuration\n",
    "        'in_channels': 3,\n",
    "        'image_size': 256,\n",
    "\n",
    "        # Standard architecture parameters\n",
    "        'embed_dim': 512,\n",
    "        'depth': 12,\n",
    "        'num_heads': 8,\n",
    "        'decoder_embed_dim': 256,\n",
    "        'decoder_depth': 4,\n",
    "        'decoder_num_heads': 8,\n",
    "        'mlp_ratio': 4,\n",
    "        'activation': 'gelu',\n",
    "        'recon_method': 'method1',\n",
    "        'dropout': 0.05,\n",
    "\n",
    "        # Additional parameters\n",
    "        'segmentation_features': True,\n",
    "        'edge_aware_loss': True\n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def get_configs():\n",
    "    configs = {\n",
    "        # Task configuration\n",
    "        'task_type': 'segmentation',  # pretext (reconstruction), segmentation_pretraining, or segmentation\n",
    "        \n",
    "        # General configs\n",
    "        'patch_size': 16,\n",
    "        'model': 'model_base',\n",
    "        'saved_fn': 'vit_autoencoder',\n",
    "        'arch': 'vit',\n",
    "        \n",
    "        # Dataset configs\n",
    "        'root_dir': 'another_one/Augmented_Dataset',\n",
    "        'split_ratios': [0.8, 0.15, 0.05],\n",
    "        'shuffle_data': True,\n",
    "        'seed': 42,\n",
    "        \n",
    "        # Training configs\n",
    "        'num_epochs': 100,\n",
    "        'start_epoch': 0,\n",
    "        'optimizer': 'adamw',\n",
    "        'device': 'cuda',\n",
    "        'loss_function': 'bce', #only works on segmentation task\n",
    "        'batch_size': 16,\n",
    "        'num_workers': 4,\n",
    "        'activation_function': 'gelu',\n",
    "        'early_stopping_patience': 20,\n",
    "        'clip_grad_norm': 1.0,\n",
    "        'use_amp': True,\n",
    "        'evaluate': False,\n",
    "        'print_freq': 50,\n",
    "        'checkpoint_freq': 5,\n",
    "        'save_best_freq': 1,\n",
    "        \n",
    "        # Learning rate configs\n",
    "        'lr': 5e-5,\n",
    "        'lr_type': 'cosine',\n",
    "        'burn_in': 10,\n",
    "        'steps': [1500, 4000],\n",
    "        'lr_step_size': 10,\n",
    "        'lr_gamma': 0.1,\n",
    "        'weight_decay': 1e-4,\n",
    "        'minimum_lr': 1e-6,\n",
    "        'milestones': [30, 60, 90],\n",
    "        'e_gamma': 0.95,\n",
    "        't_max': 10,\n",
    "        'min_lr': 1e-6,\n",
    "        'base_lr': 1e-6,\n",
    "        'max_lr': 1e-3,\n",
    "        'step_size_up': 2000,\n",
    "        'cyclic_mode': 'triangular',\n",
    "        \n",
    "        # Distributed training configs\n",
    "        'world_size': -1,\n",
    "        'rank': -1,\n",
    "        'dist_url': 'tcp://127.0.0.1:29500',\n",
    "        'dist_backend': 'nccl',\n",
    "        'gpu_idx': None,\n",
    "        'no_cuda': False,\n",
    "        'distributed': False,\n",
    "        \n",
    "        # Paths\n",
    "        'checkpoints_dir': 'checkpoints',\n",
    "        'pretrained_path': 'another_one/checkpoints/Model_vit_autoencoder_best_epoch_95_segmentation.pth',\n",
    "        'logs_dir': 'logs_4',\n",
    "        'results_dir': 'results_4',\n",
    "        'model_dir': 'model',\n",
    "        \n",
    "        # New configs\n",
    "        'early_stopping': True,\n",
    "        'patience': 15,\n",
    "        'save_checkpoint_freq': 5,\n",
    "        'step_lr_in_epoch': True,\n",
    "        'warmup_epochs': 5,\n",
    "        'normalize_data': True\n",
    "    }\n",
    "    \n",
    "    # Convert to EasyDict\n",
    "    configs = edict(configs)\n",
    "    \n",
    "    # Update with model specific configs\n",
    "    if configs.model in MODEL_CONFIGS:\n",
    "        model_config = MODEL_CONFIGS[configs.model]\n",
    "        for key, value in model_config.items():\n",
    "            setattr(configs, key, value)\n",
    "    \n",
    "    return configs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ALL THE UTILITIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mask_transform_fn(mask):\n",
    "    \"\"\"Transforms the mask by converting it into a tensor of 0s and 1s.\"\"\"\n",
    "    return torch.tensor(np.array(mask) > 0, dtype=torch.float32)\n",
    "\n",
    "def cleanup():\n",
    "    \"\"\"Clean up distributed training resources.\"\"\"\n",
    "    if dist.is_initialized():\n",
    "        dist.destroy_process_group()\n",
    "\n",
    "def calculate_metrics(outputs, masks):\n",
    "    \"\"\"\n",
    "    Calculate performance metrics for segmentation.\n",
    "    \n",
    "    Args:\n",
    "        outputs: Model predictions\n",
    "        masks: Ground truth masks\n",
    "        \n",
    "    Returns:\n",
    "        dict: Dictionary containing various metrics\n",
    "    \"\"\"\n",
    "    # Ensure inputs are properly shaped\n",
    "    if outputs.ndim == 3:\n",
    "        outputs = outputs.unsqueeze(1)\n",
    "    if masks.ndim == 3:\n",
    "        masks = masks.unsqueeze(1)\n",
    "        \n",
    "    # Convert outputs to grayscale and apply sigmoid\n",
    "    grayscale_outputs = 0.299 * outputs[:, 0, :, :] + 0.587 * outputs[:, 1, :, :] + 0.114 * outputs[:, 2, :, :]\n",
    "    grayscale_outputs = grayscale_outputs.unsqueeze(1)\n",
    "    predictions = torch.sigmoid(grayscale_outputs)\n",
    "    \n",
    "    # Threshold predictions to get binary mask\n",
    "    predictions = (predictions > 0.5).float()\n",
    "    \n",
    "    # Ensure masks are float tensors\n",
    "    masks = masks.float()\n",
    "    \n",
    "    # Ensure predictions and masks have the same shape\n",
    "    if predictions.shape != masks.shape:\n",
    "        predictions = predictions.squeeze(1)\n",
    "        masks = masks.squeeze(1)\n",
    "    \n",
    "    # Calculate intersection and union using float operations\n",
    "    intersection = (predictions * masks).sum((1, 2) if predictions.ndim == 3 else (1, 2, 3))\n",
    "    union = (predictions + masks).clamp(0, 1).sum((1, 2) if predictions.ndim == 3 else (1, 2, 3))\n",
    "    \n",
    "    # IoU (Jaccard)\n",
    "    iou = (intersection + 1e-6) / (union + 1e-6)\n",
    "    mean_iou = iou.mean().item()\n",
    "    \n",
    "    # Dice coefficient\n",
    "    dice = (2 * intersection + 1e-6) / (predictions.sum((1, 2) if predictions.ndim == 3 else (1, 2, 3)) + masks.sum((1, 2) if predictions.ndim == 3 else (1, 2, 3)) + 1e-6)\n",
    "    mean_dice = dice.mean().item()\n",
    "    \n",
    "    # Precision and Recall\n",
    "    true_positives = intersection\n",
    "    false_positives = predictions.sum((1, 2) if predictions.ndim == 3 else (1, 2, 3)) - intersection\n",
    "    false_negatives = masks.sum((1, 2) if predictions.ndim == 3 else (1, 2, 3)) - intersection\n",
    "    \n",
    "    precision = (true_positives + 1e-6) / (true_positives + false_positives + 1e-6)\n",
    "    recall = (true_positives + 1e-6) / (true_positives + false_negatives + 1e-6)\n",
    "    \n",
    "    mean_precision = precision.mean().item()\n",
    "    mean_recall = recall.mean().item()\n",
    "    \n",
    "    # F1 Score\n",
    "    f1 = 2 * (precision * recall) / (precision + recall + 1e-6)\n",
    "    mean_f1 = f1.mean().item()\n",
    "    \n",
    "    return {\n",
    "        'iou': mean_iou,\n",
    "        'dice': mean_dice,\n",
    "        'precision': mean_precision,\n",
    "        'recall': mean_recall,\n",
    "        'f1': mean_f1\n",
    "    }\n",
    "\n",
    "class Logger():\n",
    "    \"\"\"\n",
    "        Create logger to save logs during training\n",
    "        Args:\n",
    "            logs_dir:\n",
    "            saved_fn:\n",
    "\n",
    "        Returns:\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "    def __init__(self, logs_dir, saved_fn):\n",
    "        logger_fn = 'logger_{}.txt'.format(saved_fn)\n",
    "        logger_path = os.path.join(logs_dir, logger_fn)\n",
    "#/content/Self Supervised Learning - Copy/logs/logger_vit_autoencoder.txt\n",
    "        with open(logger_path, \"w\") as file:\n",
    "          pass\n",
    "\n",
    "\n",
    "        self.logger = logging.getLogger(__name__)\n",
    "        self.logger.setLevel(logging.INFO)\n",
    "\n",
    "        # formatter = logging.Formatter('%(asctime)s:File %(module)s.py:Func %(funcName)s:Line %(lineno)d:%(levelname)s: %(message)s')\n",
    "        formatter = logging.Formatter(\n",
    "            '%(asctime)s: %(module)s.py - %(funcName)s(), at Line %(lineno)d:%(levelname)s:\\n%(message)s')\n",
    "\n",
    "        file_handler = logging.FileHandler(logger_path)\n",
    "        file_handler.setLevel(logging.INFO)\n",
    "        file_handler.setFormatter(formatter)\n",
    "\n",
    "        stream_handler = logging.StreamHandler()\n",
    "        stream_handler.setFormatter(formatter)\n",
    "\n",
    "        self.logger.addHandler(file_handler)\n",
    "        self.logger.addHandler(stream_handler)\n",
    "\n",
    "    def info(self, message):\n",
    "        self.logger.info(message)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def calculate_iou(preds, targets, num_classes=1):\n",
    "    preds = preds.argmax(dim=1)  # Get the predicted class\n",
    "    iou_list = []\n",
    "    for cls in range(num_classes):\n",
    "        intersection = ((preds == cls) & (targets == cls)).sum().item()\n",
    "        union = ((preds == cls) | (targets == cls)).sum().item()\n",
    "        iou = intersection / (union + 1e-6)  # Avoid division by zero\n",
    "        iou_list.append(iou)\n",
    "    return sum(iou_list) / len(iou_list) if iou_list else 0\n",
    "\n",
    "\n",
    "def calculate_dice(preds, targets):\n",
    "    preds = preds.argmax(dim=1)  # Get the predicted class\n",
    "    intersection = (preds & targets).sum().item()\n",
    "    return (2. * intersection) / (preds.sum().item() + targets.sum().item() + 1e-6)\n",
    "\n",
    "\n",
    "def calculate_mse(preds, targets):\n",
    "    return F.mse_loss(preds, targets)\n",
    "\n",
    "\n",
    "def calculate_psnr(mse):\n",
    "    return 20 * torch.log10(1.0 / torch.sqrt(mse))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def make_folder(folder_name):\n",
    "    if not os.path.exists(folder_name):\n",
    "        os.makedirs(folder_name)\n",
    "    # or os.makedirs(folder_name, exist_ok=True)\n",
    "\n",
    "\n",
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "\n",
    "    def __init__(self, name, fmt=':f'):\n",
    "        self.name = name\n",
    "        self.fmt = fmt\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "    def __str__(self):\n",
    "        fmtstr = '{name} {val' + self.fmt + '} ({avg' + self.fmt + '})'\n",
    "        return fmtstr.format(**self.__dict__)\n",
    "\n",
    "\n",
    "class ProgressMeter(object):\n",
    "    def __init__(self, num_batches, meters, prefix=\"\"):\n",
    "        self.batch_fmtstr = self._get_batch_fmtstr(num_batches)\n",
    "        self.meters = meters\n",
    "        self.prefix = prefix\n",
    "\n",
    "    def display(self, batch):\n",
    "        entries = [self.prefix + self.batch_fmtstr.format(batch)]\n",
    "        entries += [str(meter) for meter in self.meters]\n",
    "        print('\\t'.join(entries))\n",
    "\n",
    "    def get_message(self, batch):\n",
    "        entries = [self.prefix + self.batch_fmtstr.format(batch)]\n",
    "        entries += [str(meter) for meter in self.meters]\n",
    "        return '\\t'.join(entries)\n",
    "\n",
    "    def _get_batch_fmtstr(self, num_batches):\n",
    "        num_digits = len(str(num_batches // 1))\n",
    "        fmt = '{:' + str(num_digits) + 'd}'\n",
    "        return '[' + fmt + '/' + fmt.format(num_batches) + ']'\n",
    "\n",
    "\n",
    "def time_synchronized():\n",
    "    torch.cuda.synchronize() if torch.cuda.is_available() else None\n",
    "    return time.time()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def early_stopping(val_losses, patience, logger):\n",
    "    \"\"\"\n",
    "    Implements early stopping based on validation loss.\n",
    "\n",
    "    Args:\n",
    "        val_losses (list): List of validation losses for each epoch.\n",
    "        patience (int): Number of epochs to wait before stopping if no improvement.\n",
    "\n",
    "    Returns:\n",
    "        (bool, int): (stop_training_flag, best_epoch)\n",
    "    \"\"\"\n",
    "    if len(val_losses) < patience:\n",
    "        return False, -1  # Not enough epochs to decide\n",
    "\n",
    "    min_loss = min(val_losses)\n",
    "    best_epoch = val_losses.index(min_loss) + 1  # Convert to 1-based index\n",
    "\n",
    "    # If the best validation loss didn't change in the last `patience` epochs, stop training\n",
    "    if best_epoch < len(val_losses) - patience:\n",
    "        logger.info(\n",
    "            f\"Early stopping triggered at epoch {len(val_losses)}. Best epoch: {best_epoch}\")\n",
    "        return True, best_epoch\n",
    "    return False, best_epoch\n",
    "\n",
    "\n",
    "def save_best_model(model, optimizer, lr_scheduler, epoch, best_loss, val_loss, model_type, logger, configs):\n",
    "    \"\"\"\n",
    "    Saves the best model checkpoint based on validation loss and deletes previous models.\n",
    "\n",
    "    Args:\n",
    "        model (torch.nn.Module): The model being trained.\n",
    "        optimizer (torch.optim.Optimizer): The optimizer used for training.\n",
    "        lr_scheduler (torch.optim.lr_scheduler): The learning rate scheduler.\n",
    "        epoch (int): Current epoch number.\n",
    "        best_loss (float): The best validation loss recorded.\n",
    "        val_loss (float): The current validation loss.\n",
    "        configs (Namespace): Configuration parameters.\n",
    "    \"\"\"\n",
    "    if val_loss < best_loss:\n",
    "        best_loss = val_loss\n",
    "        model_state_dict, utils_state_dict = get_saved_state(\n",
    "            model, optimizer, lr_scheduler, epoch, configs\n",
    "        )\n",
    "\n",
    "        model_save_path = os.path.join(\n",
    "            configs.model_dir, f'Model_{configs.saved_fn}_epoch_{epoch}_'+model_type+'.pth')\n",
    "        utils_save_path = os.path.join(\n",
    "            configs.model_dir, f'Utils_{configs.saved_fn}_epoch_{epoch}_'+model_type+'.pth')\n",
    "\n",
    "        # Save the new best model\n",
    "        torch.save(model_state_dict, model_save_path)\n",
    "        torch.save(utils_state_dict, utils_save_path)\n",
    "        logger.info(\n",
    "            f\"New best model saved at epoch {epoch} with val_loss: {val_loss:.4f}\")\n",
    "\n",
    "        # Delete previous models to save space\n",
    "        for file in os.listdir(configs.model_dir):\n",
    "            if file.startswith('Model_') and file != os.path.basename(model_save_path):\n",
    "                os.remove(os.path.join(configs.model_dir, file))\n",
    "            if file.startswith('Utils_') and file != os.path.basename(utils_save_path):\n",
    "                os.remove(os.path.join(configs.model_dir, file))\n",
    "                logger.info(f\"Deleted previous model checkpoint: {file}\")\n",
    "\n",
    "    return best_loss\n",
    "\n",
    "\n",
    "def get_saved_state(model, optimizer, lr_scheduler, epoch, configs):\n",
    "    \"\"\"Get the information to save with checkpoints\"\"\"\n",
    "    if hasattr(model, 'module'):\n",
    "        model_state_dict = model.module.state_dict()\n",
    "    else:\n",
    "        model_state_dict = model.state_dict()\n",
    "    utils_state_dict = {\n",
    "        'epoch': epoch,\n",
    "        'configs': configs,\n",
    "        'optimizer': copy.deepcopy(optimizer.state_dict()),\n",
    "        'lr_scheduler': copy.deepcopy(lr_scheduler.state_dict())\n",
    "    }\n",
    "\n",
    "    return model_state_dict, utils_state_dict\n",
    "\n",
    "\n",
    "def save_checkpoint(checkpoints_dir, saved_fn, model_state_dict, utils_state_dict, epoch, model_type):\n",
    "    \"\"\"Save checkpoint every epoch only is best model or after every checkpoint_freq epoch\"\"\"\n",
    "\n",
    "    model_save_path = os.path.join(\n",
    "        checkpoints_dir, f'Model_{saved_fn}_epoch_{epoch}_'+model_type+'.pth')\n",
    "    utils_save_path = os.path.join(\n",
    "        checkpoints_dir, f'Utils_{saved_fn}_epoch_{epoch}_'+model_type+'.pth')\n",
    "\n",
    "    torch.save(model_state_dict, model_save_path)\n",
    "    torch.save(utils_state_dict, utils_save_path)\n",
    "\n",
    "    print('save a checkpoint at {}'.format(model_save_path))\n",
    "\n",
    "\n",
    "def reduce_tensor(tensor, world_size):\n",
    "    rt = tensor.clone()\n",
    "    dist.all_reduce(rt, op=dist.reduce_op.SUM)\n",
    "    rt /= world_size\n",
    "    return rt\n",
    "\n",
    "\n",
    "def to_python_float(t):\n",
    "    if hasattr(t, 'item'):\n",
    "        return t.item()\n",
    "    else:\n",
    "        return t[0]\n",
    "\n",
    "\n",
    "def get_tensorboard_log(model):\n",
    "    if hasattr(model, 'module'):\n",
    "        yolo_layers = model.module.yolo_layers\n",
    "    else:\n",
    "        yolo_layers = model.yolo_layers\n",
    "\n",
    "    tensorboard_log = {}\n",
    "    tensorboard_log['Average_All_Layers'] = {}\n",
    "    for idx, yolo_layer in enumerate(yolo_layers, start=1):\n",
    "        layer_name = 'YOLO_Layer{}'.format(idx)\n",
    "        tensorboard_log[layer_name] = {}\n",
    "        for name, metric in yolo_layer.metrics.items():\n",
    "            tensorboard_log[layer_name]['{}'.format(name)] = metric\n",
    "            if idx == 1:\n",
    "                tensorboard_log['Average_All_Layers']['{}'.format(\n",
    "                    name)] = metric / len(yolo_layers)\n",
    "            else:\n",
    "                tensorboard_log['Average_All_Layers']['{}'.format(\n",
    "                    name)] += metric / len(yolo_layers)\n",
    "\n",
    "    return tensorboard_log\n",
    "\n",
    "\n",
    "def plot_lr_scheduler(optimizer, scheduler, num_epochs=300, save_dir=''):\n",
    "    # Plot LR simulating training for full num_epochs\n",
    "    optimizer, scheduler = copy.copy(optimizer), copy.copy(\n",
    "        scheduler)  # do not modify originals\n",
    "    y = []\n",
    "    for _ in range(num_epochs):\n",
    "        scheduler.step()\n",
    "        y.append(optimizer.param_groups[0]['lr'])\n",
    "    plt.plot(y, '.-', label='LR')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.ylabel('LR')\n",
    "    plt.grid()\n",
    "    plt.xlim(0, num_epochs)\n",
    "    plt.ylim(0)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(save_dir, 'LR.png'), dpi=200)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_loss_function(loss_name, smooth=1e-6):\n",
    "    \"\"\"\n",
    "    Get a loss function by name.\n",
    "\n",
    "    Args:\n",
    "        loss_name (str): Name of the loss function\n",
    "        smooth (float): Smoothing factor for dice loss\n",
    "\n",
    "    Returns:\n",
    "        callable: Loss function\n",
    "    \"\"\"\n",
    "\n",
    "    loss_functions = {\n",
    "        'mse': nn.MSELoss(),\n",
    "        'l1': nn.L1Loss(),\n",
    "        'bce': nn.BCEWithLogitsLoss(),\n",
    "        'perceptual': PerceptualLoss(),\n",
    "        'ssim': SSIMLoss(),\n",
    "    }\n",
    "\n",
    "    # Special cases that need custom handling\n",
    "    if loss_name == 'dice':\n",
    "        def dice_loss(pred, target, smooth=smooth):\n",
    "            pred = torch.sigmoid(pred)\n",
    "            pred_flat = pred.view(-1)\n",
    "            target_flat = target.view(-1)\n",
    "            intersection = (pred_flat * target_flat).sum()\n",
    "            return 1 - ((2. * intersection + smooth) / (pred_flat.sum() + target_flat.sum() + smooth))\n",
    "        return dice_loss\n",
    "    \n",
    "    elif loss_name == 'combined':\n",
    "        return CombinedLoss(alpha=0.8, beta=0.2, gamma=0.1)\n",
    "    \n",
    "    elif loss_name == 'l1_ssim':\n",
    "        # Combination of L1 and SSIM\n",
    "        l1_loss = nn.L1Loss()\n",
    "        ssim_loss = SSIMLoss()\n",
    "        def l1_ssim_loss(pred, target):\n",
    "            return 0.5 * l1_loss(pred, target) + 0.5 * ssim_loss(pred, target)\n",
    "        return l1_ssim_loss\n",
    "\n",
    "    # Return the loss function if it exists, otherwise default to combined loss\n",
    "    if loss_name in loss_functions:\n",
    "        return loss_functions[loss_name]\n",
    "    \n",
    "    else:\n",
    "        print(\n",
    "            f\"Unknown loss function: {loss_name}, using combined loss instead\")\n",
    "        return CombinedLoss(alpha=0.8, beta=0.2, gamma=0.1)\n",
    "\n",
    "\n",
    "def get_activation_function(activation_name):\n",
    "    \"\"\"\n",
    "    Get an activation function by name.\n",
    "\n",
    "    Args:\n",
    "        activation_name (str): Name of the activation function\n",
    "\n",
    "    Returns:\n",
    "        nn.Module: Activation function\n",
    "    \"\"\"\n",
    "    activations = {\n",
    "        \"relu\": nn.ReLU(),\n",
    "        \"gelu\": nn.GELU(),\n",
    "        \"sigmoid\": nn.Sigmoid(),\n",
    "        \"tanh\": nn.Tanh(),\n",
    "        \"leaky_relu\": nn.LeakyReLU(),\n",
    "        \"silu\": nn.SiLU(),  # Added SiLU/Swish activation\n",
    "        \"mish\": nn.Mish(),  # Added Mish activation\n",
    "    }\n",
    "    return activations.get(activation_name.lower(), nn.ReLU())\n",
    "\n",
    "\n",
    "def get_optimizer(optimizer_name, model_params, learning_rate=1e-4, weight_decay=1e-5, momentum=0.9):\n",
    "    \"\"\"\n",
    "    Get an optimizer by name.\n",
    "\n",
    "    Args:\n",
    "        optimizer_name (str): Name of the optimizer\n",
    "        model_params (iterable): Model parameters to optimize\n",
    "        learning_rate (float): Learning rate\n",
    "        weight_decay (float): Weight decay factor\n",
    "        momentum (float): Momentum factor for SGD\n",
    "\n",
    "    Returns:\n",
    "        torch.optim.Optimizer: Optimizer\n",
    "    \"\"\"\n",
    "    optimizer_name = optimizer_name.lower()\n",
    "\n",
    "    if optimizer_name == \"sgd\":\n",
    "        return optim.SGD(model_params, lr=learning_rate, weight_decay=weight_decay, momentum=momentum)\n",
    "    elif optimizer_name == \"adam\":\n",
    "        return optim.Adam(model_params, lr=learning_rate, weight_decay=weight_decay)\n",
    "    elif optimizer_name == \"adamw\":\n",
    "        return optim.AdamW(model_params, lr=learning_rate, weight_decay=weight_decay)\n",
    "    elif optimizer_name == \"rmsprop\":\n",
    "        return optim.RMSprop(model_params, lr=learning_rate, weight_decay=weight_decay)\n",
    "    elif optimizer_name == \"adagrad\":\n",
    "        return optim.Adagrad(model_params, lr=learning_rate, weight_decay=weight_decay)\n",
    "    else:\n",
    "        print(f\"Unknown optimizer: {optimizer_name}, using AdamW instead\")\n",
    "        return optim.AdamW(model_params, lr=learning_rate, weight_decay=weight_decay)\n",
    "\n",
    "\n",
    "def create_optimizer(configs, model):\n",
    "    \"\"\"\n",
    "    Create an optimizer for training a transformer-based model with parameter groups.\n",
    "\n",
    "    This function splits the model parameters into two groups:\n",
    "      1. Parameters that will receive weight decay (all except biases and normalization parameters).\n",
    "      2. Parameters that will not receive weight decay (biases and normalization parameters).\n",
    "\n",
    "    Args:\n",
    "        configs: Configuration object with optimizer settings\n",
    "        model (nn.Module): The model to optimize\n",
    "\n",
    "    Returns:\n",
    "        torch.optim.Optimizer: Configured optimizer\n",
    "    \"\"\"\n",
    "    # Retrieve the model parameters (supporting models wrapped in DistributedDataParallel)\n",
    "    if hasattr(model, 'module'):\n",
    "        params_dict = dict(model.module.named_parameters())\n",
    "    else:\n",
    "        params_dict = dict(model.named_parameters())\n",
    "\n",
    "    # Define keywords to exclude from weight decay\n",
    "    no_decay_keywords = [\"bias\", \"norm\",\n",
    "                         \"LayerNorm.weight\", \"layer_norm.weight\"]\n",
    "\n",
    "    # Split parameters into decay and no-decay groups\n",
    "    decay_params = []\n",
    "    no_decay_params = []\n",
    "\n",
    "    for k, v in params_dict.items():\n",
    "        if any(nd in k for nd in no_decay_keywords):\n",
    "            no_decay_params.append(v)\n",
    "        else:\n",
    "            decay_params.append(v)\n",
    "\n",
    "    # Create parameter groups\n",
    "    optimizer_grouped_parameters = [\n",
    "        {\"params\": decay_params, \"weight_decay\": configs.weight_decay},\n",
    "        {\"params\": no_decay_params, \"weight_decay\": 0.0},\n",
    "    ]\n",
    "\n",
    "    # Get optimizer settings\n",
    "    optimizer_name = configs.optimizer.lower()\n",
    "    learning_rate = configs.lr\n",
    "\n",
    "    # Create the optimizer\n",
    "    if optimizer_name == 'sgd':\n",
    "        optimizer = optim.SGD(\n",
    "            optimizer_grouped_parameters,\n",
    "            lr=learning_rate,\n",
    "            momentum=getattr(configs, 'momentum', 0.9),\n",
    "            nesterov=True\n",
    "        )\n",
    "    elif optimizer_name == 'adam':\n",
    "        optimizer = optim.Adam(optimizer_grouped_parameters, lr=learning_rate)\n",
    "    elif optimizer_name == 'adamw':\n",
    "        optimizer = optim.AdamW(optimizer_grouped_parameters, lr=learning_rate)\n",
    "    elif optimizer_name == 'rmsprop':\n",
    "        optimizer = optim.RMSprop(\n",
    "            optimizer_grouped_parameters, lr=learning_rate)\n",
    "    elif optimizer_name == 'adagrad':\n",
    "        optimizer = optim.Adagrad(\n",
    "            optimizer_grouped_parameters, lr=learning_rate)\n",
    "    else:\n",
    "        print(f\"Unknown optimizer: {optimizer_name}, using AdamW instead\")\n",
    "        optimizer = optim.AdamW(optimizer_grouped_parameters, lr=learning_rate)\n",
    "\n",
    "    # Print information about parameter groups\n",
    "    print(f'Optimizer: {optimizer_name}, Learning rate: {learning_rate}')\n",
    "    print(\n",
    "        f'Parameter groups: {len(decay_params)} parameters with weight decay, {len(no_decay_params)} parameters without weight decay')\n",
    "\n",
    "    return optimizer\n",
    "\n",
    "\n",
    "def create_lr_scheduler(optimizer, configs):\n",
    "    \"\"\"\n",
    "    Create a learning rate scheduler based on configuration.\n",
    "\n",
    "    Args:\n",
    "        optimizer: The optimizer to schedule\n",
    "        configs: Configuration object with scheduler settings\n",
    "\n",
    "    Returns:\n",
    "        torch.optim.lr_scheduler._LRScheduler: Learning rate scheduler\n",
    "    \"\"\"\n",
    "    scheduler_name = configs.lr_type.lower()\n",
    "\n",
    "    # Handle warmup if specified\n",
    "    if hasattr(configs, 'warmup_epochs') and configs.warmup_epochs > 0:\n",
    "        # Create warmup scheduler\n",
    "        def warmup_lambda(epoch):\n",
    "            if epoch < configs.warmup_epochs:\n",
    "                return float(epoch) / float(max(1, configs.warmup_epochs))\n",
    "            return 1.0\n",
    "\n",
    "        warmup_scheduler = optim.lr_scheduler.LambdaLR(\n",
    "            optimizer, lr_lambda=warmup_lambda)\n",
    "        print(f\"Using {configs.warmup_epochs} epochs of warmup\")\n",
    "    else:\n",
    "        warmup_scheduler = None\n",
    "\n",
    "    # Create main scheduler\n",
    "    if scheduler_name == 'multi_step':\n",
    "        def burnin_schedule(i):\n",
    "            if i < configs.burn_in:\n",
    "                factor = pow(i / configs.burn_in, 4)\n",
    "            elif i < configs.steps[0]:\n",
    "                factor = 1.0\n",
    "            elif i < configs.steps[1]:\n",
    "                factor = 0.1\n",
    "            else:\n",
    "                factor = 0.01\n",
    "            return factor\n",
    "\n",
    "        main_scheduler = optim.lr_scheduler.LambdaLR(\n",
    "            optimizer, burnin_schedule)\n",
    "\n",
    "    elif scheduler_name == 'cosin':\n",
    "        # Cosine annealing with warmup\n",
    "        def cosine_lambda(epoch):\n",
    "            if warmup_scheduler is not None and epoch < configs.warmup_epochs:\n",
    "                return warmup_scheduler.get_lr()[0] / configs.lr\n",
    "\n",
    "            # Cosine decay from https://arxiv.org/pdf/1812.01187.pdf\n",
    "            adjusted_epoch = epoch - \\\n",
    "                (configs.warmup_epochs if warmup_scheduler else 0)\n",
    "            adjusted_total = configs.num_epochs - \\\n",
    "                (configs.warmup_epochs if warmup_scheduler else 0)\n",
    "            return ((1 + math.cos(adjusted_epoch * math.pi / adjusted_total)) / 2) ** 1.0 * 0.9 + 0.1\n",
    "\n",
    "        main_scheduler = optim.lr_scheduler.LambdaLR(\n",
    "            optimizer, lr_lambda=cosine_lambda)\n",
    "\n",
    "    elif scheduler_name == 'step_lr':\n",
    "        main_scheduler = optim.lr_scheduler.StepLR(\n",
    "            optimizer, step_size=configs.lr_step_size, gamma=configs.lr_gamma)\n",
    "\n",
    "    elif scheduler_name == 'multistep_lr':\n",
    "        main_scheduler = optim.lr_scheduler.MultiStepLR(\n",
    "            optimizer, milestones=configs.milestones, gamma=configs.lr_gamma)\n",
    "\n",
    "    elif scheduler_name == 'exponential_lr':\n",
    "        main_scheduler = optim.lr_scheduler.ExponentialLR(\n",
    "            optimizer, gamma=configs.e_gamma)\n",
    "\n",
    "    elif scheduler_name == 'cosine_annealing_lr':\n",
    "        main_scheduler = optim.lr_scheduler.CosineAnnealingLR(\n",
    "            optimizer, T_max=configs.t_max, eta_min=configs.min_lr)\n",
    "\n",
    "    elif scheduler_name == 'reduce_lr_on_plateau':\n",
    "        main_scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "            optimizer, mode='min', factor=configs.lr_gamma, patience=configs.patience, min_lr=configs.min_lr)\n",
    "\n",
    "    elif scheduler_name == 'cyclic_lr':\n",
    "        main_scheduler = optim.lr_scheduler.CyclicLR(\n",
    "            optimizer, base_lr=configs.base_lr, max_lr=configs.max_lr,\n",
    "            step_size_up=configs.step_size_up, mode=configs.cyclic_mode)\n",
    "    else:\n",
    "        print(\n",
    "            f\"Unknown scheduler: {scheduler_name}, using CosineAnnealingLR instead\")\n",
    "        main_scheduler = optim.lr_scheduler.CosineAnnealingLR(\n",
    "            optimizer, T_max=configs.num_epochs, eta_min=getattr(configs, 'min_lr', 1e-6))\n",
    "\n",
    "    # If using warmup, return a SequentialLR\n",
    "    if warmup_scheduler is not None and scheduler_name != 'cosin':\n",
    "        return optim.lr_scheduler.SequentialLR(\n",
    "            optimizer,\n",
    "            schedulers=[warmup_scheduler, main_scheduler],\n",
    "            milestones=[configs.warmup_epochs]\n",
    "        )\n",
    "\n",
    "    return main_scheduler\n",
    "\n",
    "\n",
    "# Custom loss functions for image reconstruction\n",
    "class PerceptualLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Perceptual loss using VGG16 features.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, layers=[4, 9], weights=[1.0, 1.0]):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            layers (list): Indices of VGG layers to use for feature extraction\n",
    "            weights (list): Weights for each layer's contribution to the loss\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        # Load pretrained VGG16 model\n",
    "        vgg = models.vgg16(pretrained=True).features\n",
    "\n",
    "        # Create slices for feature extraction\n",
    "        self.slices = nn.ModuleList()\n",
    "        start_idx = 0\n",
    "        for end_idx in layers:\n",
    "            self.slices.append(nn.Sequential(\n",
    "                *list(vgg.children())[start_idx:end_idx]))\n",
    "            start_idx = end_idx\n",
    "\n",
    "        self.weights = weights\n",
    "\n",
    "        # Freeze parameters\n",
    "        for param in self.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        # Register mean and std for normalization\n",
    "        self.register_buffer('mean', torch.tensor(\n",
    "            [0.485, 0.456, 0.406]).view(1, 3, 1, 1))\n",
    "        self.register_buffer('std', torch.tensor(\n",
    "            [0.229, 0.224, 0.225]).view(1, 3, 1, 1))\n",
    "\n",
    "    def _normalize(self, img):\n",
    "        \"\"\"Normalize image for VGG\"\"\"\n",
    "        return (img - self.mean) / self.std\n",
    "\n",
    "    def forward(self, input, target):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            input (torch.Tensor): Predicted image\n",
    "            target (torch.Tensor): Target image\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Perceptual loss\n",
    "        \"\"\"\n",
    "        # Make sure input and target are on the same device as the model\n",
    "        device = next(self.parameters()).device\n",
    "        input = input.to(device)\n",
    "        target = target.to(device)\n",
    "\n",
    "        # Normalize inputs\n",
    "        input = self._normalize(input)\n",
    "        target = self._normalize(target)\n",
    "\n",
    "        # Extract features and compute loss\n",
    "        loss = 0.0\n",
    "        input_features = input\n",
    "        target_features = target\n",
    "\n",
    "        for i, slice in enumerate(self.slices):\n",
    "            input_features = slice(input_features)\n",
    "            target_features = slice(target_features)\n",
    "            loss += self.weights[i] * \\\n",
    "                F.mse_loss(input_features, target_features)\n",
    "\n",
    "        return loss\n",
    "\n",
    "\n",
    "class SSIMLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Structural Similarity Index (SSIM) loss.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, window_size=11, size_average=True):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            window_size (int): Size of the Gaussian window\n",
    "            size_average (bool): Whether to average the loss over spatial dimensions\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.window_size = window_size\n",
    "        self.size_average = size_average\n",
    "        self.channel = 1\n",
    "        self.window = self._create_window(window_size)\n",
    "\n",
    "    def _create_window(self, window_size):\n",
    "        \"\"\"Create a Gaussian window\"\"\"\n",
    "        _1D_window = torch.Tensor([1.0]).expand(window_size).unsqueeze(1)\n",
    "        _1D_window = _1D_window * _1D_window.t()\n",
    "        _1D_window = _1D_window / _1D_window.sum()\n",
    "        window = _1D_window.unsqueeze(0).unsqueeze(0)\n",
    "        return window\n",
    "\n",
    "    def forward(self, img1, img2):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            img1 (torch.Tensor): First image\n",
    "            img2 (torch.Tensor): Second image\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: 1 - SSIM (as a loss, lower is better)\n",
    "        \"\"\"\n",
    "        # Move window to same device as input\n",
    "        window = self.window.to(img1.device)\n",
    "        window = window.expand(\n",
    "            img1.size(1), 1, self.window_size, self.window_size)\n",
    "\n",
    "        # Calculate means\n",
    "        mu1 = F.conv2d(img1, window, padding=self.window_size //\n",
    "                       2, groups=img1.size(1))\n",
    "        mu2 = F.conv2d(img2, window, padding=self.window_size //\n",
    "                       2, groups=img2.size(1))\n",
    "\n",
    "        mu1_sq = mu1.pow(2)\n",
    "        mu2_sq = mu2.pow(2)\n",
    "        mu1_mu2 = mu1 * mu2\n",
    "\n",
    "        # Calculate variances and covariance\n",
    "        sigma1_sq = F.conv2d(\n",
    "            img1 * img1, window, padding=self.window_size//2, groups=img1.size(1)) - mu1_sq\n",
    "        sigma2_sq = F.conv2d(\n",
    "            img2 * img2, window, padding=self.window_size//2, groups=img2.size(1)) - mu2_sq\n",
    "        sigma12 = F.conv2d(\n",
    "            img1 * img2, window, padding=self.window_size//2, groups=img1.size(1)) - mu1_mu2\n",
    "\n",
    "        # Constants for stability\n",
    "        C1 = 0.01 ** 2\n",
    "        C2 = 0.03 ** 2\n",
    "\n",
    "        # Calculate SSIM\n",
    "        ssim_map = ((2 * mu1_mu2 + C1) * (2 * sigma12 + C2)) / \\\n",
    "            ((mu1_sq + mu2_sq + C1) * (sigma1_sq + sigma2_sq + C2))\n",
    "\n",
    "        # Return 1 - SSIM to convert to a loss (lower is better)\n",
    "        if self.size_average:\n",
    "            return 1 - ssim_map.mean()\n",
    "        else:\n",
    "            return 1 - ssim_map.mean(1).mean(1).mean(1)\n",
    "\n",
    "\n",
    "class CombinedLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Combined loss for image reconstruction, using L1, MSE, and perceptual losses.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, alpha=0.8, beta=0.2, gamma=0.1):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            alpha (float): Weight for L1 loss\n",
    "            beta (float): Weight for MSE loss\n",
    "            gamma (float): Weight for perceptual loss\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.alpha = alpha  # Weight for L1 loss\n",
    "        self.beta = beta    # Weight for MSE loss\n",
    "        self.gamma = gamma  # Weight for perceptual loss\n",
    "        self.l1_loss = nn.L1Loss()\n",
    "        self.mse_loss = nn.MSELoss()\n",
    "        self.perceptual_loss = PerceptualLoss()\n",
    "\n",
    "    def forward(self, pred, target):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            pred (torch.Tensor): Predicted image\n",
    "            target (torch.Tensor): Target image\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Combined loss\n",
    "        \"\"\"\n",
    "        # Compute L1 and MSE losses\n",
    "        l1 = self.l1_loss(pred, target)\n",
    "        mse = self.mse_loss(pred, target)\n",
    "\n",
    "        # Compute perceptual loss\n",
    "        perceptual = self.perceptual_loss(pred, target)\n",
    "\n",
    "        # Weighted combination\n",
    "        return self.alpha * l1 + self.beta * mse + self.gamma * perceptual\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def visualize_reconstructions(model, dataloader, device, epoch, save_dir):\n",
    "    \"\"\"\n",
    "    Visualize and save model reconstructions during training\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        # Get a batch of images\n",
    "        images, _ = next(iter(dataloader))\n",
    "        images = images.to(device)\n",
    "\n",
    "        # Generate reconstructions\n",
    "        reconstructions = model(images)\n",
    "        reconstructions = reconstructions.to(device)\n",
    "        # Create a grid of original and reconstructed images\n",
    "        comparison = torch.cat([images[:8], reconstructions[:8]])\n",
    "        grid = torchvision.utils.make_grid(comparison, nrow=8, normalize=True)\n",
    "\n",
    "        # Save the grid\n",
    "        os.makedirs(save_dir, exist_ok=True)\n",
    "        torchvision.utils.save_image(\n",
    "            grid, f\"{save_dir}/reconstruction_epoch_{epoch}.png\")\n",
    "\n",
    "        # Also save as matplotlib figure for better visualization\n",
    "        plt.figure(figsize=(20, 10))\n",
    "        plt.imshow(grid.permute(1, 2, 0).cpu().numpy())\n",
    "        plt.title(f\"Reconstructions at Epoch {epoch}\")\n",
    "        plt.axis('off')\n",
    "        plt.savefig(f\"{save_dir}/reconstruction_plot_epoch_{epoch}.png\")\n",
    "        plt.close()\n",
    "\n",
    "    model.train()\n",
    "    return reconstructions\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def denormalize(img, mean, std):\n",
    "    \"\"\"\n",
    "    Undo normalization for visualization.\n",
    "    img: (C, H, W) tensor\n",
    "    \"\"\"\n",
    "    img = img.clone()\n",
    "    for c in range(3):\n",
    "        img[c] = img[c] * std[c] + mean[c]\n",
    "    img = torch.clamp(img, 0, 1)  # Clamp to [0, 1] for safe visualization\n",
    "    return img\n",
    "\n",
    "def visualize_segmentation(model, dataloader, device, epoch, save_dir):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        images, masks = next(iter(dataloader))\n",
    "        images = images.to(device)\n",
    "        masks = masks.to(device)\n",
    "\n",
    "        # Fix masks shape\n",
    "        if masks.ndim == 3:\n",
    "            masks = masks.unsqueeze(1)\n",
    "\n",
    "        predictions = model(images)\n",
    "        grayscale_outputs = 0.299 * predictions[:, 0, :, :] + 0.587 * predictions[:, 1, :, :] + 0.114 * predictions[:, 2, :, :]\n",
    "        grayscale_outputs = grayscale_outputs.unsqueeze(1)\n",
    "        predictions = torch.sigmoid(grayscale_outputs)\n",
    "        predictions = (predictions > 0.5).float()\n",
    "\n",
    "        # Move tensors to CPU\n",
    "        images = images.cpu()\n",
    "        masks = masks.cpu()\n",
    "        predictions = predictions.cpu()\n",
    "\n",
    "        # Denormalization parameters (standard ImageNet mean/std)\n",
    "        mean = [0.485, 0.456, 0.406]\n",
    "        std = [0.229, 0.224, 0.225]\n",
    "\n",
    "        num_samples = min(8, images.size(0))\n",
    "        fig, axes = plt.subplots(num_samples, 3, figsize=(15, 5 * num_samples))\n",
    "\n",
    "        for i in range(num_samples):\n",
    "            # De-normalize input image\n",
    "            img = denormalize(images[i], mean, std)\n",
    "\n",
    "            # Input image\n",
    "            axes[i, 0].imshow(img.permute(1, 2, 0).numpy())\n",
    "            axes[i, 0].set_title(\"Input Image\")\n",
    "            axes[i, 0].axis(\"off\")\n",
    "\n",
    "            # Ground truth mask\n",
    "            axes[i, 1].imshow(masks[i, 0].numpy(), cmap=\"gray\")\n",
    "            axes[i, 1].set_title(\"Ground Truth Mask\")\n",
    "            axes[i, 1].axis(\"off\")\n",
    "\n",
    "            # Predicted mask\n",
    "            axes[i, 2].imshow(predictions[i, 0].numpy(), cmap=\"gray\")\n",
    "            axes[i, 2].set_title(\"Predicted Mask\")\n",
    "            axes[i, 2].axis(\"off\")\n",
    "\n",
    "        os.makedirs(save_dir, exist_ok=True)\n",
    "        save_path = os.path.join(save_dir, f\"segmentation_epoch_{epoch}.png\")\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(save_path)\n",
    "        plt.close()\n",
    "\n",
    "        print(f\"Segmentation visualization saved at {save_path}\")\n",
    "\n",
    "    model.train()\n",
    "\n",
    "def visualize_reconstructions_2(reconstructions, images, save_dir, batch):\n",
    "    \"\"\"\n",
    "    Visualize and save model reconstructions during testing\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create a grid of original and reconstructed images\n",
    "    comparison = torch.cat([images[:8], reconstructions[:8]])\n",
    "    grid = torchvision.utils.make_grid(comparison, nrow=8, normalize=True)\n",
    "\n",
    "    torchvision.utils.save_image(\n",
    "        grid, f\"{save_dir}/reconstruction_results_{batch}.png\")\n",
    "\n",
    "    # Also save as matplotlib figure for better visualization\n",
    "    plt.figure(figsize=(20, 10))\n",
    "    plt.imshow(grid.permute(1, 2, 0).cpu().numpy())\n",
    "    plt.title(f\"Reconstructions of test batch {batch}\")\n",
    "    plt.axis('off')\n",
    "    plt.savefig(f\"{save_dir}/reconstruction_results_{batch}.png\")\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def visualize_segmentation_2(predictions, images, masks, batch, save_dir):\n",
    "    \"\"\"\n",
    "    Visualize and save segmentation results during testing with error handling.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Fix masks shape\n",
    "        if masks.ndim == 3:\n",
    "            masks = masks.unsqueeze(1)\n",
    "\n",
    "        grayscale_outputs = 0.299 * predictions[:, 0, :, :] + 0.587 * predictions[:, 1, :, :] + 0.114 * predictions[:, 2, :, :]\n",
    "        grayscale_outputs = grayscale_outputs.unsqueeze(1)\n",
    "        predictions = torch.sigmoid(grayscale_outputs)\n",
    "        predictions = (predictions > 0.5).float()\n",
    "\n",
    "        # Move tensors to CPU\n",
    "        try:\n",
    "            images = images.cpu()\n",
    "            masks = masks.cpu()\n",
    "            predictions = predictions.cpu()\n",
    "        except Exception as e:\n",
    "            print(f\"Error moving tensors to CPU: {e}\")\n",
    "            return\n",
    "\n",
    "        # Denormalization parameters (standard ImageNet mean/std)\n",
    "        mean = [0.485, 0.456, 0.406]\n",
    "        std = [0.229, 0.224, 0.225]\n",
    "\n",
    "        num_samples = min(16, images.size(0))\n",
    "        try:\n",
    "            fig, axes = plt.subplots(num_samples, 3, figsize=(15, 5 * num_samples))\n",
    "        except Exception as e:\n",
    "            print(f\"Error creating subplot figure: {e}\")\n",
    "            return\n",
    "\n",
    "        try:\n",
    "            for i in range(num_samples):\n",
    "                # De-normalize input image\n",
    "                img = denormalize(images[i], mean, std)\n",
    "\n",
    "                # Input image\n",
    "                axes[i, 0].imshow(img.permute(1, 2, 0).numpy())\n",
    "                axes[i, 0].set_title(\"Input Image\")\n",
    "                axes[i, 0].axis(\"off\")\n",
    "\n",
    "                # Ground truth mask\n",
    "                axes[i, 1].imshow(masks[i, 0].numpy(), cmap=\"gray\")\n",
    "                axes[i, 1].set_title(\"Ground Truth Mask\")\n",
    "                axes[i, 1].axis(\"off\")\n",
    "\n",
    "                # Predicted mask\n",
    "                axes[i, 2].imshow(predictions[i, 0].numpy(), cmap=\"gray\")\n",
    "                axes[i, 2].set_title(\"Predicted Mask\")\n",
    "                axes[i, 2].axis(\"off\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error plotting images: {e}\")\n",
    "            plt.close(fig)\n",
    "            return\n",
    "\n",
    "        try:\n",
    "            os.makedirs(save_dir, exist_ok=True)\n",
    "            save_path = os.path.join(save_dir, f\"segmentation_result_batch_{batch}.png\")\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(save_path)\n",
    "            plt.close(fig)\n",
    "        except Exception as e:\n",
    "            print(f\"Error saving visualization: {e}\")\n",
    "            plt.close(fig)\n",
    "            return\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Unexpected error in visualize_segmentation_2: {e}\")\n",
    "        # Ensure figure is closed in case of error\n",
    "        try:\n",
    "            plt.close()\n",
    "        except:\n",
    "            pass\n",
    "        return\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DATALOADER AND DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageMaskDataset(Dataset):\n",
    "    def __init__(self, root_dir, image_size=(256, 256)):\n",
    "        \"\"\"\n",
    "        Dataset class for loading images and corresponding masks.\n",
    "        Args:\n",
    "        - root_dir (str): Root directory containing images and masks\n",
    "        - image_size (tuple[int, int]): Tuple specifying image resize dimensions\n",
    "        \"\"\"\n",
    "        assert os.path.exists(root_dir), f\"Error: Directory '{root_dir}' does not exist.\"\n",
    "        \n",
    "        self.image_paths, self.mask_paths = self._load_paths(root_dir)\n",
    "        self.valid_pairs = self._validate_pairs()\n",
    "        print(f\"Found {len(self.valid_pairs)} valid image-mask pairs\")\n",
    "\n",
    "        self.image_transform = transforms.Compose([\n",
    "            transforms.Resize(image_size),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "\n",
    "        self.mask_transform = transforms.Compose([\n",
    "            transforms.Resize(image_size),\n",
    "            transforms.Lambda(mask_transform_fn)\n",
    "        ])\n",
    "\n",
    "    \n",
    "    def _load_paths(self, root_dir):\n",
    "        \"\"\"\n",
    "        Load paths for images and their corresponding masks from multiple dataset directories.\n",
    "        \n",
    "        Args:\n",
    "            root_dir (str): Root directory containing multiple dataset folders\n",
    "            \n",
    "        Returns:\n",
    "            tuple[list, list]: Lists of image and mask file paths\n",
    "        \"\"\"\n",
    "        image_paths = []\n",
    "        mask_paths = []\n",
    "        \n",
    "        # Get all dataset directories\n",
    "        dataset_dirs = [d for d in os.listdir(root_dir) if os.path.isdir(os.path.join(root_dir, d))]\n",
    "        \n",
    "        for dataset_dir in dataset_dirs:\n",
    "            dataset_path = os.path.join(root_dir, dataset_dir)\n",
    "            images_dir = os.path.join(dataset_path, 'images')\n",
    "            masks_dir = os.path.join(dataset_path, 'masks')\n",
    "            \n",
    "            # Skip if images or masks directory doesn't exist\n",
    "            if not os.path.exists(images_dir) or not os.path.exists(masks_dir):\n",
    "                print(f\"Warning: Skipping {dataset_dir} - missing images or masks directory\")\n",
    "                continue\n",
    "            \n",
    "            # Get list of image files\n",
    "            image_files = sorted([f for f in os.listdir(images_dir) \n",
    "                                if f.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp'))])\n",
    "            \n",
    "            # Get list of mask files\n",
    "            mask_files = sorted([f for f in os.listdir(masks_dir)\n",
    "                            if f.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp'))])\n",
    "            \n",
    "            # Verify matching numbers of images and masks\n",
    "            if len(image_files) != len(mask_files):\n",
    "                print(f\"Warning: Skipping {dataset_dir} - number of images ({len(image_files)}) \"\n",
    "                    f\"does not match number of masks ({len(mask_files)})\")\n",
    "                continue\n",
    "            \n",
    "            # Add full paths to lists\n",
    "            image_paths.extend([os.path.join(images_dir, f) for f in image_files])\n",
    "            mask_paths.extend([os.path.join(masks_dir, f) for f in mask_files])\n",
    "            \n",
    "            print(f\"Added {len(image_files)} pairs from {dataset_dir}\")\n",
    "        \n",
    "        if not image_paths:\n",
    "            raise ValueError(f\"No valid image-mask pairs found in {root_dir}\")\n",
    "        \n",
    "        print(f\"Found total of {len(image_paths)} image-mask pairs across all datasets\")\n",
    "        return image_paths, mask_paths\n",
    "\n",
    "\n",
    "    def _validate_image(self, image_path):\n",
    "        \"\"\"Validate if an image file is readable\"\"\"\n",
    "        try:\n",
    "            with Image.open(image_path) as img:\n",
    "                img.verify()\n",
    "            return True\n",
    "        except:\n",
    "            print(f\"Warning: Corrupted or unreadable image file: {image_path}\")\n",
    "            return False\n",
    "\n",
    "    def _validate_pairs(self):\n",
    "        \"\"\"Validate all image-mask pairs and return only valid ones\"\"\"\n",
    "        valid_pairs = []\n",
    "        for img_path, mask_path in tqdm(zip(self.image_paths, self.mask_paths), \n",
    "                                      desc=\"Validating image-mask pairs\",\n",
    "                                      total=len(self.image_paths)):\n",
    "            if self._validate_image(img_path) and self._validate_image(mask_path):\n",
    "                valid_pairs.append((img_path, mask_path))\n",
    "            else:\n",
    "                print(f\"Skipping corrupted pair:\\nImage: {img_path}\\nMask: {mask_path}\\n\")\n",
    "        return valid_pairs\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Return a single image-mask pair.\n",
    "        Args:\n",
    "        - idx (int): Index of the desired image-mask pair\n",
    "        Returns:\n",
    "        - tuple[torch.Tensor, torch.Tensor]: Transformed image and mask\n",
    "        \"\"\"\n",
    "        try:\n",
    "            image_path, mask_path = self.valid_pairs[idx]\n",
    "            \n",
    "            # Load and convert image\n",
    "            with Image.open(image_path) as image:\n",
    "                image = image.convert(\"RGB\")\n",
    "                image_tensor = self.image_transform(image)\n",
    "\n",
    "            # Load and convert mask\n",
    "            with Image.open(mask_path) as mask:\n",
    "                mask = mask.convert(\"L\")\n",
    "                mask_tensor = self.mask_transform(mask)\n",
    "\n",
    "            return image_tensor, mask_tensor\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading pair {idx}:\\nImage: {image_path}\\nMask: {mask_path}\\nError: {str(e)}\")\n",
    "            # Return a zero tensor of appropriate size as fallback\n",
    "            return torch.zeros(3, 256, 256), torch.zeros(256, 256)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.valid_pairs)\n",
    "    \n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class DatasetLoader:\n",
    "    def __init__(self, image_size, root_dir, split_ratios=[0.7, 0.15, 0.15], seed=42, \n",
    "                 Shuffle=True, batch_size=16, num_workers=2, task='pretext'):\n",
    "        \"\"\"\n",
    "        Initialize DatasetLoader with optional augmentation.\n",
    "        \n",
    "        Args:\n",
    "            augment (string): Whether to apply augmentation to the dataset depeding on the task type (pretext or downstream)\n",
    "        \"\"\"\n",
    "        self.image_size = image_size\n",
    "        self.task = task\n",
    "        \n",
    "        if task == 'pretext':\n",
    "            # Perform augmentation before creating the dataset\n",
    "            self.augment_dataset(root_dir)\n",
    "            # Use augmented directory for dataset\n",
    "            self.dataset = ImageMaskDataset(\n",
    "                os.path.join(root_dir, 'augmented'), (image_size, image_size))\n",
    "        else:\n",
    "            self.dataset = ImageMaskDataset(root_dir, (image_size, image_size))\n",
    "            \n",
    "        self.split_ratios = split_ratios\n",
    "        self.seed = seed\n",
    "        self.shuffle_data = Shuffle\n",
    "        self.batch_size = batch_size\n",
    "        self.num_workers = num_workers\n",
    "\n",
    "    def augment_dataset(self, root_dir):\n",
    "        \"\"\"Apply augmentation to the dataset.\"\"\"\n",
    "        # Create augmented directory\n",
    "        augmented_dir = os.path.join(root_dir, 'augmented')\n",
    "        os.makedirs(augmented_dir, exist_ok=True)\n",
    "        \n",
    "        # Parameters for augmentation\n",
    "        angles = [45, 90, 270]\n",
    "        scale_factors = [0.7, 1.4]\n",
    "        shift_values = [(25, 0), (-25, 0), (0, 25), (0, -25)]\n",
    "        flip_codes = [0, 1, -1]\n",
    "        target_resolution = (768, 576)\n",
    "\n",
    "        for dataset_dir in os.listdir(root_dir):\n",
    "            dataset_path = os.path.join(root_dir, dataset_dir)\n",
    "            if not os.path.isdir(dataset_path) or dataset_dir == 'augmented':\n",
    "                continue\n",
    "\n",
    "            images_dir = os.path.join(dataset_path, 'images')\n",
    "            masks_dir = os.path.join(dataset_path, 'masks')\n",
    "            \n",
    "            if not (os.path.exists(images_dir) and os.path.exists(masks_dir)):\n",
    "                continue\n",
    "\n",
    "            # Create output directories for augmented data\n",
    "            aug_dataset_dir = os.path.join(augmented_dir, dataset_dir)\n",
    "            aug_images_dir = os.path.join(aug_dataset_dir, 'images')\n",
    "            aug_masks_dir = os.path.join(aug_dataset_dir, 'masks')\n",
    "            os.makedirs(aug_images_dir, exist_ok=True)\n",
    "            os.makedirs(aug_masks_dir, exist_ok=True)\n",
    "\n",
    "            # Copy original images and masks\n",
    "            for img_name in os.listdir(images_dir):\n",
    "                if img_name.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
    "                    # Load image and mask\n",
    "                    img_path = os.path.join(images_dir, img_name)\n",
    "                    mask_path = os.path.join(masks_dir, img_name)\n",
    "                    \n",
    "                    if not os.path.exists(mask_path):\n",
    "                        continue\n",
    "                        \n",
    "                    image = cv2.imread(img_path)\n",
    "                    mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)\n",
    "                    \n",
    "                    # Save original\n",
    "                    cv2.imwrite(os.path.join(aug_images_dir, img_name), image)\n",
    "                    cv2.imwrite(os.path.join(aug_masks_dir, img_name), mask)\n",
    "                    \n",
    "                    # Rotations\n",
    "                    for angle in angles:\n",
    "                        rotated_img, rotated_mask = self.rotate_image_and_mask(\n",
    "                            image, mask, angle, target_resolution)\n",
    "                        cv2.imwrite(os.path.join(aug_images_dir, \n",
    "                            f\"{os.path.splitext(img_name)[0]}_rot{angle}.jpg\"), rotated_img)\n",
    "                        cv2.imwrite(os.path.join(aug_masks_dir, \n",
    "                            f\"{os.path.splitext(img_name)[0]}_rot{angle}.png\"), rotated_mask)\n",
    "                    \n",
    "                    # Scaling\n",
    "                    for scale in scale_factors:\n",
    "                        scaled_img, scaled_mask = self.scale_image_and_mask(\n",
    "                            image, mask, scale, target_resolution)\n",
    "                        cv2.imwrite(os.path.join(aug_images_dir, \n",
    "                            f\"{os.path.splitext(img_name)[0]}_scale{scale}.jpg\"), scaled_img)\n",
    "                        cv2.imwrite(os.path.join(aug_masks_dir, \n",
    "                            f\"{os.path.splitext(img_name)[0]}_scale{scale}.png\"), scaled_mask)\n",
    "                    \n",
    "                    # Shifts\n",
    "                    for shift_x, shift_y in shift_values:\n",
    "                        shifted_img, shifted_mask = self.shift_image_and_mask(\n",
    "                            image, mask, shift_x, shift_y, target_resolution)\n",
    "                        cv2.imwrite(os.path.join(aug_images_dir, \n",
    "                            f\"{os.path.splitext(img_name)[0]}_shift{shift_x}_{shift_y}.jpg\"), shifted_img)\n",
    "                        cv2.imwrite(os.path.join(aug_masks_dir, \n",
    "                            f\"{os.path.splitext(img_name)[0]}_shift{shift_x}_{shift_y}.png\"), shifted_mask)\n",
    "                    \n",
    "                    # Flips\n",
    "                    for flip_code in flip_codes:\n",
    "                        flipped_img, flipped_mask = self.flip_image_and_mask(\n",
    "                            image, mask, flip_code, target_resolution)\n",
    "                        cv2.imwrite(os.path.join(aug_images_dir, \n",
    "                            f\"{os.path.splitext(img_name)[0]}_flip{flip_code}.jpg\"), flipped_img)\n",
    "                        cv2.imwrite(os.path.join(aug_masks_dir, \n",
    "                            f\"{os.path.splitext(img_name)[0]}_flip{flip_code}.png\"), flipped_mask)\n",
    "\n",
    "    @staticmethod\n",
    "    def rotate_image_and_mask(image, mask, angle, target_resolution):\n",
    "        # Implementation of rotate_image_and_mask function\n",
    "        if angle == 90:\n",
    "            rotated_image = cv2.rotate(image, cv2.ROTATE_90_CLOCKWISE)\n",
    "            rotated_mask = cv2.rotate(mask, cv2.ROTATE_90_CLOCKWISE)\n",
    "        elif angle == 180:\n",
    "            rotated_image = cv2.rotate(image, cv2.ROTATE_180)\n",
    "            rotated_mask = cv2.rotate(mask, cv2.ROTATE_180)\n",
    "        elif angle == 270:\n",
    "            rotated_image = cv2.rotate(image, cv2.ROTATE_90_COUNTERCLOCKWISE)\n",
    "            rotated_mask = cv2.rotate(mask, cv2.ROTATE_90_COUNTERCLOCKWISE)\n",
    "        elif angle == 45:\n",
    "            (h, w) = image.shape[:2]\n",
    "            diag_length = int(np.sqrt(h**2 + w**2))\n",
    "            padding = (diag_length - h) // 2\n",
    "\n",
    "            padded_image = cv2.copyMakeBorder(image, padding, padding, padding, padding, \n",
    "                                            borderType=cv2.BORDER_REPLICATE)\n",
    "            padded_mask = cv2.copyMakeBorder(mask, padding, padding, padding, padding, \n",
    "                                           borderType=cv2.BORDER_REPLICATE)\n",
    "\n",
    "            (ph, pw) = padded_image.shape[:2]\n",
    "            center = (pw // 2, ph // 2)\n",
    "            M = cv2.getRotationMatrix2D(center, angle, 1.0)\n",
    "            rotated_image = cv2.warpAffine(padded_image, M, (pw, ph), \n",
    "                flags=cv2.INTER_LINEAR, borderMode=cv2.BORDER_CONSTANT, borderValue=(255,255,255))\n",
    "            rotated_mask = cv2.warpAffine(padded_mask, M, (pw, ph), \n",
    "                flags=cv2.INTER_NEAREST, borderMode=cv2.BORDER_CONSTANT, borderValue=0)\n",
    "\n",
    "            start_x, start_y = (pw - w) // 2, (ph - h) // 2\n",
    "            rotated_image = rotated_image[start_y:start_y + h, start_x:start_x + w]\n",
    "            rotated_mask = rotated_mask[start_y:start_y + h, start_x:start_x + w]\n",
    "        else:\n",
    "            rotated_image = image.copy()\n",
    "            rotated_mask = mask.copy()\n",
    "\n",
    "        resized_image = cv2.resize(rotated_image, target_resolution, interpolation=cv2.INTER_LINEAR)\n",
    "        resized_mask = cv2.resize(rotated_mask, target_resolution, interpolation=cv2.INTER_NEAREST)\n",
    "        \n",
    "        return resized_image, resized_mask\n",
    "\n",
    "    @staticmethod\n",
    "    def scale_image_and_mask(image, mask, scale_factor, target_resolution):\n",
    "        scaled_image = cv2.resize(image, None, fx=scale_factor, fy=scale_factor, \n",
    "                                interpolation=cv2.INTER_LINEAR)\n",
    "        scaled_mask = cv2.resize(mask, None, fx=scale_factor, fy=scale_factor, \n",
    "                               interpolation=cv2.INTER_NEAREST)\n",
    "        \n",
    "        resized_image = cv2.resize(scaled_image, target_resolution, interpolation=cv2.INTER_LINEAR)\n",
    "        resized_mask = cv2.resize(scaled_mask, target_resolution, interpolation=cv2.INTER_NEAREST)\n",
    "        \n",
    "        return resized_image, resized_mask\n",
    "\n",
    "    @staticmethod\n",
    "    def shift_image_and_mask(image, mask, shift_x, shift_y, target_resolution):\n",
    "        M = np.float32([[1, 0, shift_x], [0, 1, shift_y]])\n",
    "        shifted_image = cv2.warpAffine(image, M, (image.shape[1], image.shape[0]), \n",
    "            borderMode=cv2.BORDER_CONSTANT, borderValue=(255,255,255))\n",
    "        shifted_mask = cv2.warpAffine(mask, M, (mask.shape[1], mask.shape[0]), \n",
    "            borderMode=cv2.BORDER_CONSTANT, borderValue=0)\n",
    "        \n",
    "        resized_image = cv2.resize(shifted_image, target_resolution, interpolation=cv2.INTER_LINEAR)\n",
    "        resized_mask = cv2.resize(shifted_mask, target_resolution, interpolation=cv2.INTER_NEAREST)\n",
    "        \n",
    "        return resized_image, resized_mask\n",
    "\n",
    "    @staticmethod\n",
    "    def flip_image_and_mask(image, mask, flip_code, target_resolution):\n",
    "        flipped_image = cv2.flip(image, flip_code)\n",
    "        flipped_mask = cv2.flip(mask, flip_code)\n",
    "        \n",
    "        resized_image = cv2.resize(flipped_image, target_resolution, interpolation=cv2.INTER_LINEAR)\n",
    "        resized_mask = cv2.resize(flipped_mask, target_resolution, interpolation=cv2.INTER_NEAREST)\n",
    "        \n",
    "        return resized_image, resized_mask\n",
    "\n",
    "    def get_dataloaders(self):\n",
    "        \"\"\"Split dataset and return DataLoaders for train, val, and test.\"\"\"\n",
    "        # Rest of the method remains unchanged\n",
    "        dataset_size = len(self.dataset)\n",
    "        train_size = int(dataset_size * self.split_ratios[0])\n",
    "        val_size = int(dataset_size * self.split_ratios[1])\n",
    "        test_size = dataset_size - train_size - val_size\n",
    "\n",
    "        generator = torch.Generator().manual_seed(self.seed)\n",
    "\n",
    "        if self.shuffle_data:\n",
    "            train_dataset, val_dataset, test_dataset = random_split(\n",
    "                self.dataset, [train_size, val_size, test_size], generator=generator)\n",
    "        else:\n",
    "            train_dataset = torch.utils.data.Subset(\n",
    "                self.dataset, range(0, train_size))\n",
    "            val_dataset = torch.utils.data.Subset(\n",
    "                self.dataset, range(train_size, train_size + val_size))\n",
    "            test_dataset = torch.utils.data.Subset(\n",
    "                self.dataset, range(train_size + val_size, dataset_size))\n",
    "\n",
    "        print(\"Creating DataLoaders...\")\n",
    "        train_loader = DataLoader(\n",
    "            train_dataset, batch_size=self.batch_size, shuffle=True, num_workers=self.num_workers)\n",
    "        val_loader = DataLoader(\n",
    "            val_dataset, batch_size=self.batch_size, shuffle=False, num_workers=self.num_workers)\n",
    "        test_loader = DataLoader(\n",
    "            test_dataset, batch_size=self.batch_size, shuffle=False, num_workers=self.num_workers)\n",
    "\n",
    "        return train_loader, val_loader, test_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MODEL CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PATCH EMBEDDING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchEmbedding(nn.Module):\n",
    "    def __init__(self, image_size=256, patch_size=16, in_channels=3, embed_dim=1024):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            image_size (int): \"The spatial size of the input image (assumed square, default is 256).\"\n",
    "            patch_size (int): \"The size of each patch along both height and width dimensions (default is 16).\"\n",
    "            in_channels (int): \"Number of input channels in the image (default is 3 for RGB images).\"\n",
    "            embed_dim (int): \"The size of the embedding dimension for each patch (default is 1024).\"\n",
    "        \"\"\"\n",
    "        super(PatchEmbedding, self).__init__()\n",
    "\n",
    "        self.image_size = image_size\n",
    "        self.patch_size = patch_size\n",
    "        self.in_channels = in_channels\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_patches_h = image_size // patch_size\n",
    "        self.num_patches_w = image_size // patch_size\n",
    "        # calculating total number of patches\n",
    "        self.num_patches = self.num_patches_h * self.num_patches_w\n",
    "        \n",
    "        # Conv projection of the patches\n",
    "        self.proj = nn.Conv2d(\n",
    "            in_channels=in_channels,\n",
    "            out_channels=embed_dim,\n",
    "            kernel_size=patch_size,\n",
    "            stride=patch_size\n",
    "        )\n",
    "        \n",
    "        self.pos_embed = nn.Parameter(torch.randn(\n",
    "            1, self.num_patches, embed_dim))  # Positional Embedding\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x (torch.Tensor): \"A 4D input tensor of shape (B, C, H, W), where:\n",
    "                               B is batch size,\n",
    "                               C is the number of channels,\n",
    "                               H and W are the height and width of the image.\"\n",
    "        Returns:\n",
    "            torch.Tensor: \"A tensor of shape (B, num_patches, embed_dim) containing the projected patch embeddings with positional embeddings added.\"\n",
    "        \"\"\"\n",
    "        # Apply conv projection\n",
    "        x = self.proj(x)\n",
    "        \n",
    "        # Reshape using einops: (B, embed_dim, H', W') -> (B, num_patches, embed_dim)\n",
    "        x = einops.rearrange(x, 'b e h w -> b (h w) e')\n",
    "        \n",
    "        # Add positional embeddings\n",
    "        x = x + self.pos_embed\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CUSTOM TRANSFORMER BLOCK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomTransformerBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Custom transformer encoder block with multi-head self-attention and a feed-forward network.\n",
    "\n",
    "    Args:\n",
    "        embed_dim (int): The embedding dimension of the input.\n",
    "        num_heads (int): The number of attention heads.\n",
    "        mlp_ratio (float): The ratio of the size of the feed-forward network to the embedding size.\n",
    "        activation (str): The activation function to use in the feed-forward network.\n",
    "        is_decoder(bool): Whether this block is part of a decoder.\n",
    "\n",
    "    Methods:\n",
    "        forward(src, src_mask=None, src_key_padding_mask=None):\n",
    "            Args:\n",
    "                src (torch.Tensor): The input tensor.\n",
    "                src_mask (torch.Tensor, optional): The mask for the input tensor.\n",
    "                src_key_padding_mask (torch.Tensor, optional): The key padding mask for the input tensor.\n",
    "\n",
    "            Returns:\n",
    "                torch.Tensor: The output tensor after self-attention and feed-forward pass.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, embed_dim, num_heads, mlp_ratio, activation, dropout, is_decoder=False):\n",
    "        super(CustomTransformerBlock, self).__init__()\n",
    "\n",
    "        self.self_attn = nn.MultiheadAttention(embed_dim, num_heads)\n",
    "        \n",
    "        self.norm1 = nn.LayerNorm(embed_dim)\n",
    "        self.norm2 = nn.LayerNorm(embed_dim)\n",
    "        \n",
    "        self.is_decoder = is_decoder\n",
    "        \n",
    "        if is_decoder:\n",
    "            self.cross_attn = nn.MultiheadAttention(embed_dim, num_heads)\n",
    "            self.norm3 = nn.LayerNorm(embed_dim)\n",
    "\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(embed_dim, int(embed_dim * mlp_ratio)),\n",
    "            get_activation_function(activation),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(int(embed_dim * mlp_ratio), embed_dim),\n",
    "            nn.Dropout(dropout)            \n",
    "        )\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, src, memory=None, src_mask=None, memory_mask=None,src_key_padding_mask=None, memory_key_padding_mask=None):\n",
    "        \"\"\"\n",
    "        Forward pass for the encoder block.\n",
    "\n",
    "        Args:\n",
    "            src (torch.Tensor): The input tensor.\n",
    "            src_mask (torch.Tensor, optional): The mask for the input tensor.\n",
    "            src_key_padding_mask (torch.Tensor, optional): The key padding mask for the input tensor.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: The output tensor after self-attention and feed-forward pass.\n",
    "        \"\"\"\n",
    "        \n",
    "        src = self.norm1(src)  # Layer normalization\n",
    "        \n",
    "        attn_output, _ = self.self_attn(\n",
    "            src, src, src, attn_mask=src_mask, key_padding_mask=src_key_padding_mask)\n",
    "        \n",
    "        src = src + self.dropout(attn_output)  # Add residual connection\n",
    "        \n",
    "        src = self.norm2(src)  # Layer normalization\n",
    "        \n",
    "        if self.is_decoder:\n",
    "            cross_output, _ = self.cross_attn(src, memory, memory, attn_mask=memory_mask, key_padding_mask=memory_key_padding_mask)\n",
    "            src = src + self.dropout(cross_output)\n",
    "            src = self.norm3(src)  # Layer normalization\n",
    "            \n",
    "        ffn_output = self.ffn(src)\n",
    "        \n",
    "        return ffn_output\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ENCODER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, embed_dim=1024, depth=24, num_heads=16, mlp_ratio=4, activation='gelu', dropout=0.1):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            embed_dim (int): \"Dimension of the input embedding for each patch (default is 1024).\"\n",
    "            depth (int): \"Number of transformer encoder layers (default is 24).\"\n",
    "            num_heads (int): \"Number of attention heads in the multi-head attention mechanism (default is 16).\"\n",
    "            mlp_ratio (int): \"Multiplier for the feedforward network dimension in each layer (default is 4).\"\n",
    "            activation (str): \"Activation function to use in the decoder layers (default is 'gelu').\"\n",
    "            Dropout (int): \"Dropout ratio (0-1) for the drop out layer (default is 0.1).\"\n",
    "        \"\"\"\n",
    "        super(Encoder, self).__init__()\n",
    "\n",
    "        self.layers = nn.ModuleList([\n",
    "            CustomTransformerBlock(\n",
    "                embed_dim, num_heads, mlp_ratio, activation, dropout)\n",
    "            for _ in range(depth)\n",
    "        ])\n",
    "\n",
    "        self.norm = nn.LayerNorm(embed_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x (torch.Tensor): \"A tensor of shape (B, num_patches, embed_dim) representing the input patch embeddings.\"\n",
    "        Returns:\n",
    "            torch.Tensor: \"A normalized tensor of the same shape after processing through the transformer encoder layers.\"\n",
    "        \"\"\"\n",
    "\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        x = self.norm(x)\n",
    "        return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RECONSTRUCTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# -*- coding: utf-8 -*-\n",
    "-----------------------------------------------------------------------------------\n",
    "# Author: Zakria Mehmood\n",
    "# DoC: 2025.04.15\n",
    "# email: zakriamehmood2001@gmail.com\n",
    "-----------------------------------------------------------------------------------\n",
    "# Description: All types of Reconstruction methods\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# linear mapping method\n",
    "\n",
    "class ReconstructionMethod1(nn.Module):\n",
    "    def __init__(self, decoder_dim, patch_size, image_size, output_channels):\n",
    "        super(ReconstructionMethod1, self).__init__()\n",
    "        self.decoder_dim = decoder_dim\n",
    "        self.patch_size = patch_size\n",
    "        self.image_size = image_size\n",
    "        self.out_channels = output_channels\n",
    "        # Total patches in the image\n",
    "        self.num_patches = (image_size // patch_size) ** 2\n",
    "\n",
    "        self.linear = nn.Linear(decoder_dim, patch_size * patch_size * output_channels, bias=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        batch_size, num_patches, _ = x.shape\n",
    "\n",
    "        if hasattr(self, 'num_patches'):\n",
    "            assert num_patches == self.num_patches, f\"Expected {self.num_patches} patches but got {num_patches}\"\n",
    "        else:\n",
    "            self.num_patches = num_patches\n",
    "            print(\n",
    "                f\"Warning: num_patches was not initialized. Setting to {num_patches} based on input.\")\n",
    "\n",
    "        x = self.linear(x)\n",
    "        \n",
    "        x = rearrange(x, 'b (h w) (p1 p2 c) -> b c (h p1) (w p2)',\n",
    "                      h=int(self.num_patches**0.5), w=int(self.num_patches**0.5),\n",
    "                      p1=self.patch_size, p2=self.patch_size, c=x.shape[-1] // (self.patch_size * self.patch_size))\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "# Convolutional Reconstruction\n",
    "class ReconstructionMethod2(nn.Module):\n",
    "    def __init__(self, decoder_dim, patch_size, output_channels):\n",
    "        super().__init__()\n",
    "        self.patch_size = patch_size\n",
    "\n",
    "        self.conv_decoder = nn.Sequential(\n",
    "            nn.Conv2d(decoder_dim, 256, 3, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(256, 128, 3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(128, output_channels, 3, padding=1),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, N, C = x.shape\n",
    "        h = w = int(N ** 0.5)\n",
    "        x = x.transpose(1, 2).view(B, C, h, w)\n",
    "        return self.conv_decoder(x)\n",
    "\n",
    "\n",
    "# Progressive Upsampling Reconstruction\n",
    "class ReconstructionMethod3(nn.Module):\n",
    "    def __init__(self, decoder_dim, patch_size, output_channels):\n",
    "        super().__init__()\n",
    "        self.patch_size = patch_size\n",
    "\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(decoder_dim, patch_size * patch_size * 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Unflatten(2, (64, patch_size, patch_size)),\n",
    "            nn.ConvTranspose2d(64, 32, 4, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(32, output_channels, 4, stride=2, padding=1),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.decoder(x)\n",
    "\n",
    "\n",
    "# Residual Reconstruction\n",
    "class ReconstructionMethod4(nn.Module):\n",
    "    def __init__(self, decoder_dim, patch_size, output_channels):\n",
    "        super().__init__()\n",
    "\n",
    "        class ResBlock(nn.Module):\n",
    "            def __init__(self, channels):\n",
    "                super().__init__()\n",
    "                self.conv = nn.Sequential(\n",
    "                    nn.Conv2d(channels, channels, 3, padding=1),\n",
    "                    nn.BatchNorm2d(channels),\n",
    "                    nn.ReLU(),\n",
    "                    nn.Conv2d(channels, channels, 3, padding=1),\n",
    "                    nn.BatchNorm2d(channels)\n",
    "                )\n",
    "\n",
    "            def forward(self, x):\n",
    "                return x + self.conv(x)\n",
    "\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(decoder_dim, patch_size * patch_size * 64),\n",
    "            nn.Unflatten(1, (64, patch_size, patch_size)),\n",
    "            ResBlock(64),\n",
    "            ResBlock(64),\n",
    "            nn.Conv2d(64, output_channels, 1),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "# Attention-guided Reconstruction\n",
    "\n",
    "\n",
    "class ReconstructionMethod5(nn.Module):\n",
    "    def __init__(self, decoder_dim, patch_size, output_channels):\n",
    "        super().__init__()\n",
    "\n",
    "        self.query = nn.Linear(decoder_dim, decoder_dim)\n",
    "        self.key = nn.Linear(decoder_dim, decoder_dim)\n",
    "        self.value = nn.Linear(decoder_dim, decoder_dim)\n",
    "\n",
    "        self.final = nn.Sequential(\n",
    "            nn.Linear(decoder_dim, patch_size * patch_size * output_channels),\n",
    "            nn.Unflatten(2, (output_channels, patch_size, patch_size))\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, N, C = x.shape\n",
    "        q = self.query(x)\n",
    "        k = self.key(x)\n",
    "        v = self.value(x)\n",
    "\n",
    "        attention = (q @ k.transpose(-2, -1)) / math.sqrt(C)\n",
    "        attention = F.softmax(attention, dim=-1)\n",
    "\n",
    "        x = attention @ v\n",
    "        return self.final(x)\n",
    "\n",
    "\n",
    "# Hybrid Reconstruction\n",
    "class ReconstructionMethod6(nn.Module):\n",
    "    def __init__(self, decoder_dim, patch_size, output_channels):\n",
    "        super().__init__()\n",
    "\n",
    "        self.transformer_part = nn.TransformerDecoderLayer(\n",
    "            d_model=decoder_dim,\n",
    "            nhead=8\n",
    "        )\n",
    "\n",
    "        self.cnn_part = nn.Sequential(\n",
    "            nn.ConvTranspose2d(decoder_dim, 256, 4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(256, output_channels, 4, stride=2, padding=1),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.transformer_part(x, x)\n",
    "        B, N, C = x.shape\n",
    "        h = w = int(math.sqrt(N))\n",
    "        x = x.transpose(1, 2).view(B, C, h, w)\n",
    "        return self.cnn_part(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DECODER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, embed_dim=1024, decoder_embed_dim=512, depth=8, num_heads=16, mlp_ratio=4, image_size=256, activation='gelu', patch_size=16, recon_method=\"method1\", dropout=0.1, output_channels=3):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            embed_dim (int): \"The embedding dimension of the encoder output (default is 1024).\"\n",
    "            decoder_embed_dim (int): \"The embedding dimension for the decoder (default is 512).\"\n",
    "            depth (int): \"Number of transformer decoder layers (default is 8).\"\n",
    "            num_heads (int): \"Number of attention heads in the multi-head attention mechanism (default is 16).\"\n",
    "            mlp_ratio (int): \"Multiplier for the feedforward network dimension in each decoder layer (default is 4).\"\n",
    "            image_size (int): \"The size of the reconstructed image (default is 256).\"\n",
    "            activation (str): \"Activation function to use in the decoder layers (default is 'gelu').\"\n",
    "            patch_size (int): \"The size of each patch along both height and width dimensions (default is 16).\"\n",
    "            recon_method (str): \"The selection of reconstruction method (default is 'method1'). \n",
    "                                 Use 'method4' for segmentation-optimized reconstruction.\"\n",
    "            Dropout (int): \"Dropout ratio (0-1) for the drop out layer (default is 0.1).\"\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        # Projection from encoder embedding dimension to decoder embedding dimension\n",
    "        self.decoder_embed = nn.Linear(embed_dim, decoder_embed_dim)\n",
    "\n",
    "        # Position embeddings for decoder\n",
    "        self.decoder_pos_embed = nn.Parameter(torch.zeros(\n",
    "            1, (image_size // patch_size) ** 2, decoder_embed_dim))\n",
    "\n",
    "        # Decoder blocks\n",
    "        self.decoder_blocks = nn.ModuleList([\n",
    "            CustomTransformerBlock(\n",
    "                embed_dim=decoder_embed_dim,\n",
    "                num_heads=num_heads,\n",
    "                mlp_ratio=mlp_ratio,\n",
    "                activation=activation,\n",
    "                dropout=dropout,\n",
    "                is_decoder=True\n",
    "            )\n",
    "            for _ in range(depth)\n",
    "        ])\n",
    "        \n",
    "        # Normalization layer\n",
    "        self.decoder_norm = nn.LayerNorm(decoder_embed_dim)\n",
    "\n",
    "        # Store the reconstruction method name\n",
    "        self.recon_method = recon_method\n",
    "\n",
    "        # Select reconstruction method based on the argument\n",
    "        if recon_method == \"method1\":\n",
    "            self.reconstruction = ReconstructionMethod1(\n",
    "                decoder_dim=decoder_embed_dim,\n",
    "                patch_size=patch_size,\n",
    "                image_size=image_size,\n",
    "                output_channels=output_channels\n",
    "            )\n",
    "        elif recon_method == \"method2\":\n",
    "            self.reconstruction = ReconstructionMethod2(\n",
    "                decoder_dim=decoder_embed_dim,\n",
    "                patch_size=patch_size,\n",
    "                output_channels=output_channels\n",
    "            )\n",
    "        elif recon_method == \"method3\":\n",
    "            self.reconstruction = ReconstructionMethod3(\n",
    "                decoder_dim=decoder_embed_dim,\n",
    "                patch_size=patch_size,\n",
    "                output_channels=output_channels\n",
    "            )\n",
    "        elif recon_method == \"method4\":\n",
    "            # Segmentation-optimized reconstruction method\n",
    "            self.reconstruction = ReconstructionMethod4(\n",
    "                decoder_dim=decoder_embed_dim,\n",
    "                patch_size=patch_size,\n",
    "                output_channels=output_channels\n",
    "            )\n",
    "        elif recon_method == \"method5\":\n",
    "            self.reconstruction = ReconstructionMethod5(\n",
    "                decoder_dim=decoder_embed_dim,\n",
    "                patch_size=patch_size,\n",
    "                output_channels=output_channels\n",
    "            )\n",
    "        elif recon_method == \"method6\":\n",
    "            self.reconstruction = ReconstructionMethod6(\n",
    "                decoder_dim=decoder_embed_dim,\n",
    "                patch_size=patch_size,\n",
    "                output_channels=output_channels\n",
    "            )\n",
    "\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                f\"Unknown reconstruction method: {recon_method}. Choose from 'method1', 'method2', 'method3', 'method4', 'method5' or 'method6'.\")\n",
    "\n",
    "\n",
    "    def forward(self, x, memory=None):\n",
    "        # Project from encoder dimension to decoder dimension\n",
    "        x = self.decoder_embed(x)\n",
    "\n",
    "        # Add position embeddings\n",
    "        x = x + self.decoder_pos_embed\n",
    "\n",
    "        # Apply decoder blocks\n",
    "        for block in self.decoder_blocks:\n",
    "            x = block(x, memory) if memory is not None else block(x, x)\n",
    "\n",
    "        # Apply normalization\n",
    "        x = self.decoder_norm(x)\n",
    "\n",
    "        # Apply reconstruction method to get the final output\n",
    "        output = self.reconstruction(x)\n",
    "\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model BASE CLASS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VisionTransformerAutoencoder(nn.Module):\n",
    "    def __init__(self, image_size=256, patch_size=16, in_channels=3, embed_dim=512, depth=12,\n",
    "                 num_heads=8, decoder_embed_dim=256, decoder_depth=4, decoder_num_heads=8,\n",
    "                 mlp_ratio=4, activation='gelu', recon_method=\"method3\", dropout=0.1, output_channels=3):\n",
    "        \"\"\"\n",
    "        \n",
    "        Args:\n",
    "            image_size (int): \"The input image height and width, assumed to be square (default is 256).\"\n",
    "            patch_size (int): \"The size of each patch extracted from the image (default is 16).\"\n",
    "            in_channels (int): \"Number of input image channels (default is 3 for RGB).\"\n",
    "            embed_dim (int): \"Embedding dimension for patch embeddings in the encoder (default is 512).\"\n",
    "            depth (int): \"Number of transformer encoder layers (default is 12).\"\n",
    "            num_heads (int): \"Number of attention heads for the encoder (default is 8).\"\n",
    "            decoder_embed_dim (int): \"Embedding dimension for the decoder (default is 256).\"\n",
    "            decoder_depth (int): \"Number of transformer decoder layers (default is 4).\"\n",
    "            decoder_num_heads (int): \"Number of attention heads for the decoder (default is 8).\"\n",
    "            mlp_ratio (int): \"Multiplier for the feedforward network dimension in transformer layers (default is 4).\"\n",
    "            activation (str): \"Activation function to use in the decoder layers (default is 'gelu').\"\n",
    "            recon_method (str): \"The selection of reconstruction method (default is 'method3').\"\n",
    "            Dropout (int): \"Dropout ratio (0-1) for the drop out layer (default is 0.1).\"\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        assert (image_size % patch_size == 0) and (image_size % patch_size == 0), \\\n",
    "            f\"Image dimensions ({image_size}x{image_size}) must be divisible by patch size ({patch_size})\"\n",
    "\n",
    "        # Patch embedding\n",
    "        self.patch_embed = PatchEmbedding(\n",
    "            image_size=image_size,\n",
    "            patch_size=patch_size,\n",
    "            in_channels=in_channels,\n",
    "            embed_dim=embed_dim\n",
    "        )\n",
    "\n",
    "        # Encoder\n",
    "        self.encoder = Encoder(\n",
    "            embed_dim=embed_dim,\n",
    "            depth=depth,\n",
    "            num_heads=num_heads,\n",
    "            mlp_ratio=mlp_ratio,\n",
    "            activation=activation,\n",
    "            dropout=dropout\n",
    "        )\n",
    "\n",
    "        # Decoder\n",
    "        self.decoder = Decoder(\n",
    "            embed_dim=embed_dim,\n",
    "            decoder_embed_dim=decoder_embed_dim,\n",
    "            depth=decoder_depth,\n",
    "            num_heads=decoder_num_heads,\n",
    "            mlp_ratio=mlp_ratio,\n",
    "            image_size=image_size,\n",
    "            activation=activation,\n",
    "            patch_size=patch_size,\n",
    "            recon_method=recon_method,\n",
    "            dropout=dropout,\n",
    "            output_channels=output_channels\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x (torch.Tensor): \"Input image tensor of shape (B, C, H, W), where B is batch size, C is number of channels, H and W are image dimensions.\"\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: \"Reconstructed image tensor of shape (B, 3, image_size, image_size).\"\n",
    "        \"\"\"\n",
    "        # Patch embedding\n",
    "        x = self.patch_embed(x)\n",
    "\n",
    "        # Encoder\n",
    "        x = self.encoder(x)\n",
    "\n",
    "        # Decoder\n",
    "        x = self.decoder(x)\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## model utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_weights(model):\n",
    "    \"\"\"\n",
    "    Initialize all model weights using Xavier uniform initialization.\n",
    "    \n",
    "    Args:\n",
    "        model (nn.Module): The model to initialize\n",
    "        \n",
    "    Returns:\n",
    "        model (nn.Module): The initialized model\n",
    "    \"\"\"\n",
    "    for m in model.modules():\n",
    "        if isinstance(m, nn.Linear):\n",
    "            # Xavier uniform for all linear layers\n",
    "            nn.init.xavier_uniform_(m.weight)\n",
    "            if m.bias is not None:\n",
    "                nn.init.zeros_(m.bias)\n",
    "                \n",
    "        elif isinstance(m, (nn.LayerNorm, nn.BatchNorm2d)):\n",
    "            # Standard initialization for normalization layers\n",
    "            nn.init.ones_(m.weight)\n",
    "            nn.init.zeros_(m.bias)\n",
    "            \n",
    "        elif isinstance(m, nn.Conv2d):\n",
    "            # Xavier uniform for all convolutional layers\n",
    "            nn.init.xavier_uniform_(m.weight)\n",
    "            if m.bias is not None:\n",
    "                nn.init.zeros_(m.bias)\n",
    "                \n",
    "        elif isinstance(m, nn.ConvTranspose2d):\n",
    "            # Xavier uniform for transpose convolutions\n",
    "            nn.init.xavier_uniform_(m.weight)\n",
    "            if m.bias is not None:\n",
    "                nn.init.zeros_(m.bias)\n",
    "                \n",
    "        elif isinstance(m, nn.MultiheadAttention):\n",
    "            # Xavier uniform for attention layers\n",
    "            if hasattr(m, 'in_proj_weight') and m.in_proj_weight is not None:\n",
    "                nn.init.xavier_uniform_(m.in_proj_weight)\n",
    "            \n",
    "            # Initialize out projection\n",
    "            if hasattr(m, 'out_proj') and hasattr(m.out_proj, 'weight'):\n",
    "                nn.init.xavier_uniform_(m.out_proj.weight)\n",
    "                if m.out_proj.bias is not None:\n",
    "                    nn.init.zeros_(m.out_proj.bias)\n",
    "            \n",
    "            # Initialize separate Q, K, V projections if they exist\n",
    "            for weight_name in ['q_proj_weight', 'k_proj_weight', 'v_proj_weight']:\n",
    "                if hasattr(m, weight_name) and getattr(m, weight_name) is not None:\n",
    "                    weight = getattr(m, weight_name)\n",
    "                    nn.init.xavier_uniform_(weight)\n",
    "\n",
    "    # Initialize positional embeddings if they exist\n",
    "    if hasattr(model, 'patch_embed') and hasattr(model.patch_embed, 'pos_embed'):\n",
    "        nn.init.xavier_uniform_(model.patch_embed.pos_embed)\n",
    "    \n",
    "    # Initialize decoder positional embeddings if they exist\n",
    "    if hasattr(model, 'decoder') and hasattr(model.decoder, 'decoder_pos_embed'):\n",
    "        nn.init.xavier_uniform_(model.decoder.decoder_pos_embed)\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def verify_initialization(model):\n",
    "    \"\"\"\n",
    "    Verify that the model weights are properly initialized using Xavier uniform initialization.\n",
    "    \n",
    "    Args:\n",
    "        model: The model to verify\n",
    "        \n",
    "    Returns:\n",
    "        bool: True if initialization is correct, False otherwise\n",
    "    \"\"\"\n",
    "    initialization_ok = True\n",
    "    \n",
    "    for name, param in model.named_parameters():\n",
    "        if param.requires_grad:\n",
    "            if 'weight' in name and param.dim() >= 2:\n",
    "                # Calculate expected Xavier bounds\n",
    "                fan_in = param.size(1)\n",
    "                fan_out = param.size(0)\n",
    "                std = math.sqrt(2.0 / (fan_in + fan_out))\n",
    "                bound = math.sqrt(3.0) * std\n",
    "                \n",
    "                # Check if weights are within Xavier bounds\n",
    "                if torch.any(torch.abs(param) > bound):\n",
    "                    print(f\"Warning: {name} has weights outside Xavier bounds (-{bound:.4f}, {bound:.4f})\")\n",
    "                    initialization_ok = False\n",
    "                \n",
    "                # Check weight distribution\n",
    "                actual_std = torch.std(param).item()\n",
    "                expected_std = std\n",
    "                if not (0.7 * expected_std <= actual_std <= 1.3 * expected_std):\n",
    "                    print(f\"Warning: {name} has unexpected standard deviation: {actual_std:.4f} (expected  {expected_std:.4f})\")\n",
    "                    initialization_ok = False\n",
    "                \n",
    "                # Check for dead neurons\n",
    "                if torch.any(torch.all(param == 0, dim=1)):\n",
    "                    print(f\"Warning: {name} has dead neurons (all-zero weights)\")\n",
    "                    initialization_ok = False\n",
    "            \n",
    "            # Check normalization layers\n",
    "            elif any(x in name for x in ['norm', 'ln', 'batch_norm']) and 'weight' in name:\n",
    "                if not torch.allclose(param, torch.ones_like(param), rtol=1e-3):\n",
    "                    print(f\"Warning: {name} normalization weights not initialized to ones\")\n",
    "                    initialization_ok = False\n",
    "            \n",
    "            # Verify bias initialization\n",
    "            elif 'bias' in name:\n",
    "                if not torch.allclose(param, torch.zeros_like(param), rtol=1e-3):\n",
    "                    print(f\"Warning: {name} bias not initialized to zeros\")\n",
    "                    initialization_ok = False\n",
    "    \n",
    "    # Verify positional embeddings\n",
    "    if hasattr(model, 'patch_embed') and hasattr(model.patch_embed, 'pos_embed'):\n",
    "        pos_embed = model.patch_embed.pos_embed\n",
    "        fan_in = pos_embed.size(-1)\n",
    "        fan_out = pos_embed.size(-2)\n",
    "        std = math.sqrt(2.0 / (fan_in + fan_out))\n",
    "        bound = math.sqrt(3.0) * std\n",
    "        \n",
    "        if torch.any(torch.abs(pos_embed) > bound):\n",
    "            print(f\"Warning: Positional embeddings outside Xavier bounds (-{bound:.4f}, {bound:.4f})\")\n",
    "            initialization_ok = False\n",
    "    \n",
    "    # Verify decoder positional embeddings\n",
    "    if hasattr(model, 'decoder') and hasattr(model.decoder, 'decoder_pos_embed'):\n",
    "        dec_pos_embed = model.decoder.decoder_pos_embed\n",
    "        fan_in = dec_pos_embed.size(-1)\n",
    "        fan_out = dec_pos_embed.size(-2)\n",
    "        std = math.sqrt(2.0 / (fan_in + fan_out))\n",
    "        bound = math.sqrt(3.0) * std\n",
    "        \n",
    "        if torch.any(torch.abs(dec_pos_embed) > bound):\n",
    "            print(f\"Warning: Decoder positional embeddings outside Xavier bounds (-{bound:.4f}, {bound:.4f})\")\n",
    "            initialization_ok = False\n",
    "    \n",
    "    if initialization_ok:\n",
    "        print(\"All weights properly initialized with Xavier uniform distribution!\")\n",
    "        print(f\"Note: Weights are bounded by their respective fan-in/fan-out values\")\n",
    "    \n",
    "    return initialization_ok\n",
    "\n",
    "\n",
    "\n",
    "def log_initialization(model, logger=None):\n",
    "    \"\"\"\n",
    "    Log information about the initialized model.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): The initialized model\n",
    "        logger: Logger object to log information\n",
    "    \"\"\"\n",
    "    # Count total parameters\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    trainable_params = sum(p.numel()\n",
    "                           for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "    # Log initialization info\n",
    "    if logger:\n",
    "        logger.info(f\"Model initialized with Xavier weights\")\n",
    "        logger.info(f\"Total parameters: {total_params:,}\")\n",
    "        logger.info(f\"Trainable parameters: {trainable_params:,}\")\n",
    "\n",
    "        # Log model structure\n",
    "        logger.info(\"Model structure:\")\n",
    "        logger.info(\n",
    "            f\"- Patch Embedding: {model.patch_embed.__class__.__name__}\")\n",
    "        logger.info(\n",
    "            f\"- Encoder: {model.encoder.__class__.__name__} with {len(model.encoder.layers)} layers\")\n",
    "        logger.info(\n",
    "            f\"- Decoder: {model.decoder.__class__.__name__} with {len(model.decoder.decoder_blocks)} layers\")\n",
    "        if hasattr(model.decoder, 'reconstruction'):\n",
    "            logger.info(\n",
    "                f\"- Reconstruction method: {model.decoder.reconstruction.__class__.__name__}\")\n",
    "    else:\n",
    "        print(f\"Model initialized with Xavier weights\")\n",
    "        print(f\"Total parameters: {total_params:,}\")\n",
    "        print(f\"Trainable parameters: {trainable_params:,}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def create_model(configs, logger=None):\n",
    "    \"\"\"\n",
    "    Create model based on architecture name and return the model and appropriate loss function.\n",
    "\n",
    "    Args:\n",
    "        configs: Configuration object with model parameters\n",
    "        logger: Optional logger for logging information\n",
    "\n",
    "    Returns:\n",
    "        tuple: (model, loss_function) - The initialized model and appropriate loss function\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    network = MODEL_CONFIGS['model_base']\n",
    "    # Check if the model is a vision transformer\n",
    "\n",
    "    if logger is not None:\n",
    "        logger.info('Using vision transformer...')\n",
    "        logger.info('Model Network: {}'.format(network))\n",
    "\n",
    "    # Get reconstruction method from config or use default\n",
    "    recon_method = network.get('recon_method', 'method1')\n",
    "\n",
    "    output_channels = 3  # Default output channels for RGB images\n",
    "\n",
    "    model = VisionTransformerAutoencoder(configs.image_size,\n",
    "                                            configs.patch_size,\n",
    "                                            configs.in_channels,\n",
    "                                            int(network['embed_dim']),\n",
    "                                            int(network['depth']),\n",
    "                                            int(network['num_heads']),\n",
    "                                            int(network['decoder_embed_dim']),\n",
    "                                            int(network['decoder_depth']),\n",
    "                                            int(network['decoder_num_heads']),\n",
    "                                            int(network['mlp_ratio']),\n",
    "                                            network['activation'],\n",
    "                                            recon_method,\n",
    "                                            int(network['dropout']),\n",
    "                                            output_channels = output_channels)\n",
    "\n",
    "    # Initialize model weights using Xavier initialization\n",
    "    model = initialize_weights(model)\n",
    "\n",
    "    # Log initialization information\n",
    "    log_initialization(model, logger)\n",
    "\n",
    "    # Verify initialization\n",
    "    verify_initialization(model)\n",
    "\n",
    "    # Create appropriate loss function\n",
    "    if configs.task_type == 'pretext':\n",
    "        loss_fn = create_reconstruction_loss(recon_method)\n",
    "    else:\n",
    "        loss_fn = get_loss_function(configs.loss_function)\n",
    "\n",
    "    if logger is not None:\n",
    "        logger.info(f'Using reconstruction method: {recon_method}')\n",
    "        logger.info(f'Using loss function: {configs.loss_function}')\n",
    "        logger.info(f\"Model architecture: {model.__class__.__name__}\")\n",
    "    \n",
    "    logger.info(f\"Total parameters: {get_num_parameters(model):,}\")\n",
    "\n",
    "    return model, loss_fn\n",
    "\n",
    "\n",
    "def get_num_parameters(model):\n",
    "    \"\"\"Count number of trained parameters of the model\"\"\"\n",
    "    if hasattr(model, 'module'):\n",
    "        num_parameters = sum(p.numel()\n",
    "                             for p in model.module.parameters() if p.requires_grad)\n",
    "    else:\n",
    "        num_parameters = sum(p.numel()\n",
    "                             for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "    return num_parameters\n",
    "\n",
    "\n",
    "def make_data_parallel(model, configs):\n",
    "    if configs.distributed:\n",
    "        # For multiprocessing distributed, DistributedDataParallel constructor\n",
    "        # should always set the single device scope, otherwise,\n",
    "        # DistributedDataParallel will use all available devices.\n",
    "        if configs.gpu_idx is not None:\n",
    "            torch.cuda.set_device(configs.gpu_idx)\n",
    "            model.cuda(configs.gpu_idx)\n",
    "            # When using a single GPU per process and per\n",
    "            # DistributedDataParallel, we need to divide the batch size\n",
    "            # ourselves based on the total number of GPUs we have\n",
    "            configs.batch_size = int(\n",
    "                configs.batch_size / configs.ngpus_per_node)\n",
    "            configs.num_workers = int(\n",
    "                (configs.num_workers + configs.ngpus_per_node - 1) / configs.ngpus_per_node)\n",
    "            model = torch.nn.parallel.DistributedDataParallel(\n",
    "                model, device_ids=[configs.gpu_idx])\n",
    "        else:\n",
    "            model.cuda()\n",
    "            # DistributedDataParallel will divide and allocate batch_size to all\n",
    "            # available GPUs if device_ids are not set\n",
    "            model = torch.nn.parallel.DistributedDataParallel(model)\n",
    "    elif configs.gpu_idx is not None:\n",
    "        torch.cuda.set_device(configs.gpu_idx)\n",
    "        model = model.cuda(configs.gpu_idx)\n",
    "    else:\n",
    "        # DataParallel will divide and allocate batch_size to all available GPUs\n",
    "        model = torch.nn.DataParallel(model).cuda()\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def create_reconstruction_loss(recon_method=\"method1\"):\n",
    "    \"\"\"\n",
    "    Create a loss function suitable for the specified reconstruction method.\n",
    "\n",
    "    Args:\n",
    "        recon_method (str): The reconstruction method being used\n",
    "\n",
    "    Returns:\n",
    "        function: A loss function that takes model output and target as input\n",
    "    \"\"\"\n",
    "    if recon_method == \"method4\":\n",
    "        # For segmentation-optimized reconstruction (method4)\n",
    "        def segmentation_pretraining_loss(output, target):\n",
    "            # Unpack outputs if in training mode\n",
    "            if isinstance(output, tuple):\n",
    "                recon_img, semantic_features = output\n",
    "\n",
    "                # Reconstruction loss (L1 loss for better edge preservation)\n",
    "                recon_loss = nn.functional.l1_loss(recon_img, target)\n",
    "\n",
    "                # Feature consistency loss (encourage similar features for similar regions)\n",
    "                # This helps the model learn semantic representations useful for segmentation\n",
    "                # We use a simple proxy by computing gradients in the target image\n",
    "                target_gray = 0.299 * \\\n",
    "                    target[:, 0] + 0.587 * target[:, 1] + 0.114 * target[:, 2]\n",
    "                target_gray = target_gray.unsqueeze(1)  # Add channel dimension\n",
    "\n",
    "                # Compute gradients using Sobel filters\n",
    "                sobel_x = torch.tensor([[-1, 0, 1], [-2, 0, 2], [-1, 0, 1]],\n",
    "                                       dtype=torch.float32, device=target.device).view(1, 1, 3, 3)\n",
    "                sobel_y = torch.tensor([[-1, -2, -1], [0, 0, 0], [1, 2, 1]],\n",
    "                                       dtype=torch.float32, device=target.device).view(1, 1, 3, 3)\n",
    "\n",
    "                # Apply filters\n",
    "                grad_x = nn.functional.conv2d(target_gray, sobel_x, padding=1)\n",
    "                grad_y = nn.functional.conv2d(target_gray, sobel_y, padding=1)\n",
    "                grad_magnitude = torch.sqrt(grad_x**2 + grad_y**2)\n",
    "\n",
    "                # Normalize gradient magnitude to [0, 1]\n",
    "                grad_magnitude = grad_magnitude / \\\n",
    "                    (torch.max(grad_magnitude) + 1e-8)\n",
    "\n",
    "                # Feature consistency loss - higher weight on edge regions\n",
    "                edge_weight = 1.0 + 5.0 * grad_magnitude  # Higher weight on edges\n",
    "                feature_loss = torch.mean(edge_weight * torch.abs(\n",
    "                    nn.functional.normalize(semantic_features, dim=1) -\n",
    "                    nn.functional.normalize(grad_magnitude, dim=1)\n",
    "                ))\n",
    "\n",
    "                # Total loss with weighting\n",
    "                total_loss = recon_loss + 0.5 * feature_loss\n",
    "                return total_loss\n",
    "            else:\n",
    "                # If not in training mode, just use L1 loss\n",
    "                return nn.functional.l1_loss(output, target)\n",
    "\n",
    "        return segmentation_pretraining_loss\n",
    "    else:\n",
    "        # For other reconstruction methods, use a combination of L1 and SSIM loss\n",
    "        def standard_reconstruction_loss(output, target):\n",
    "            # L1 loss for pixel-wise accuracy\n",
    "            l1_loss = nn.functional.l1_loss(output, target)\n",
    "\n",
    "            # MSE loss for overall image similarity\n",
    "            mse_loss = nn.functional.mse_loss(output, target)\n",
    "\n",
    "            # Combined loss\n",
    "            return 0.8 * l1_loss + 0.2 * mse_loss\n",
    "\n",
    "        return standard_reconstruction_loss\n",
    "\n",
    "\n",
    "def transfer_to_segmentation_model(pretrained_model, num_classes, logger=None):\n",
    "    \"\"\"\n",
    "    Transfer a pretrained VisionTransformerAutoencoder model to a segmentation model.\n",
    "\n",
    "    Args:\n",
    "        pretrained_model (nn.Module): The pretrained VisionTransformerAutoencoder model\n",
    "        num_classes (int): Number of segmentation classes\n",
    "        logger: Optional logger for logging information\n",
    "\n",
    "    Returns:\n",
    "        nn.Module: A segmentation model initialized with pretrained weights\n",
    "    \"\"\"\n",
    "    # Create a new model that reuses the encoder and patch embedding\n",
    "    class SegmentationModel(nn.Module):\n",
    "        def __init__(self, pretrained_vit, num_classes):\n",
    "            super().__init__()\n",
    "\n",
    "            # Reuse patch embedding from pretrained model\n",
    "            self.patch_embed = pretrained_vit.patch_embed\n",
    "\n",
    "            # Reuse encoder from pretrained model\n",
    "            self.encoder = pretrained_vit.encoder\n",
    "\n",
    "            # Get dimensions from pretrained model\n",
    "            self.embed_dim = next(pretrained_vit.encoder.parameters()).size(1)\n",
    "            self.patch_size = pretrained_vit.patch_embed.patch_size\n",
    "            self.image_size = pretrained_vit.patch_embed.image_size\n",
    "            self.num_patches = pretrained_vit.patch_embed.num_patches\n",
    "\n",
    "            # Create segmentation head\n",
    "            self.segmentation_head = nn.Sequential(\n",
    "                nn.Linear(self.embed_dim, 256),\n",
    "                nn.GELU(),\n",
    "                nn.Linear(256, num_classes * self.patch_size * self.patch_size)\n",
    "            )\n",
    "\n",
    "            # Final upsampling and refinement\n",
    "            self.refine = nn.Sequential(\n",
    "                nn.Conv2d(num_classes, 64, kernel_size=3, padding=1),\n",
    "                nn.BatchNorm2d(64),\n",
    "                nn.ReLU(),\n",
    "                nn.Conv2d(64, 32, kernel_size=3, padding=1),\n",
    "                nn.BatchNorm2d(32),\n",
    "                nn.ReLU(),\n",
    "                nn.Conv2d(32, num_classes, kernel_size=1)\n",
    "            )\n",
    "\n",
    "        def forward(self, x):\n",
    "            # Get patches\n",
    "            x = self.patch_embed(x)\n",
    "\n",
    "            # Encode patches\n",
    "            x = self.encoder(x)\n",
    "\n",
    "            # Apply segmentation head\n",
    "            batch_size = x.shape[0]\n",
    "            # [B, num_patches, num_classes * patch_size^2]\n",
    "            x = self.segmentation_head(x)\n",
    "\n",
    "            # Reshape to image format\n",
    "            patches_per_side = int(math.sqrt(self.num_patches))\n",
    "            x = x.reshape(batch_size, patches_per_side, patches_per_side,\n",
    "                          num_classes, self.patch_size, self.patch_size)\n",
    "            x = x.permute(0, 3, 1, 4, 2, 5)\n",
    "            x = x.reshape(batch_size, num_classes,\n",
    "                          self.image_size, self.image_size)\n",
    "\n",
    "            # Apply refinement\n",
    "            x = self.refine(x)\n",
    "\n",
    "            return x\n",
    "\n",
    "    # Create the segmentation model\n",
    "    seg_model = SegmentationModel(pretrained_model, num_classes)\n",
    "\n",
    "    # Initialize new layers with Xavier initialization\n",
    "    for m in seg_model.modules():\n",
    "        if isinstance(m, nn.Linear) and m not in pretrained_model.modules():\n",
    "            nn.init.xavier_uniform_(m.weight)\n",
    "            if m.bias is not None:\n",
    "                nn.init.zeros_(m.bias)\n",
    "        elif isinstance(m, nn.Conv2d) and m not in pretrained_model.modules():\n",
    "            nn.init.xavier_uniform_(m.weight)\n",
    "            if m.bias is not None:\n",
    "                nn.init.zeros_(m.bias)\n",
    "        elif isinstance(m, nn.BatchNorm2d) and m not in pretrained_model.modules():\n",
    "            nn.init.ones_(m.weight)\n",
    "            nn.init.zeros_(m.bias)\n",
    "\n",
    "    if logger:\n",
    "        logger.info(f\"Created segmentation model with {num_classes} classes\")\n",
    "        logger.info(\n",
    "            f\"Transferred weights from pretrained VisionTransformerAutoencoder\")\n",
    "\n",
    "        # Count parameters\n",
    "        total_params = sum(p.numel() for p in seg_model.parameters())\n",
    "        trainable_params = sum(p.numel()\n",
    "                               for p in seg_model.parameters() if p.requires_grad)\n",
    "        logger.info(f\"Total parameters: {total_params:,}\")\n",
    "        logger.info(f\"Trainable parameters: {trainable_params:,}\")\n",
    "\n",
    "    return seg_model\n",
    "\n",
    "\n",
    "def load_pretrained_for_segmentation(pretrained_path, num_classes, configs, logger=None):\n",
    "    \"\"\"\n",
    "    Load a pretrained model and convert it to a segmentation model.\n",
    "\n",
    "    Args:\n",
    "        pretrained_path (str): Path to the pretrained model checkpoint\n",
    "        num_classes (int): Number of segmentation classes\n",
    "        configs: Configuration object with model parameters\n",
    "        logger: Optional logger for logging information\n",
    "\n",
    "    Returns:\n",
    "        nn.Module: A segmentation model initialized with pretrained weights\n",
    "    \"\"\"\n",
    "    if logger:\n",
    "        logger.info(f\"Loading pretrained model from {pretrained_path}\")\n",
    "\n",
    "    # Create the base model first\n",
    "    base_model, _ = create_model(configs, logger)\n",
    "\n",
    "    # Load the pretrained weights\n",
    "    checkpoint = torch.load(pretrained_path, map_location='cpu')\n",
    "    if 'model' in checkpoint:\n",
    "        # If the checkpoint contains a 'model' key, use that\n",
    "        base_model.load_state_dict(checkpoint['model'])\n",
    "    else:\n",
    "        # Otherwise assume the checkpoint is the model state dict directly\n",
    "        base_model.load_state_dict(checkpoint)\n",
    "\n",
    "    if logger:\n",
    "        logger.info(\"Pretrained weights loaded successfully\")\n",
    "\n",
    "    # Transfer to segmentation model\n",
    "    seg_model = transfer_to_segmentation_model(base_model, num_classes, logger)\n",
    "\n",
    "    return seg_model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(train_dataloader, model, optimizer, lr_scheduler, epoch, configs, logger, tb_writer, device, scaler, loss_fn):\n",
    "\n",
    "    batch_time = AverageMeter('Time', ':6.3f')\n",
    "    data_time = AverageMeter('Data', ':6.3f')\n",
    "    losses = AverageMeter('Loss', ':.4e')\n",
    "    grad_norm = AverageMeter('GradNorm', ':.4e')\n",
    "\n",
    "    progress = ProgressMeter(len(train_dataloader), [batch_time, data_time, losses, grad_norm],\n",
    "                             prefix=\"Train - Epoch: [{}/{}]\".format(epoch, configs.num_epochs))\n",
    "\n",
    "    lr_scheduler.step()\n",
    "    current_lr = optimizer.param_groups[0]['lr']\n",
    "    logger.info(\n",
    "        f\"Epoch {epoch+1}/{configs.num_epochs}: Learning rate = {current_lr:.8f}\")\n",
    "\n",
    "    num_iters_per_epoch = len(train_dataloader)\n",
    "\n",
    "    # switch to train mode\n",
    "    model.train()\n",
    "    start_time = time.time()\n",
    "\n",
    "    for batch_idx, (imgs, masks) in enumerate(train_dataloader):\n",
    "        # Move data to the correct device\n",
    "        imgs = imgs.to(device)\n",
    "        masks = masks.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(imgs)\n",
    "\n",
    "        # Handle the case where outputs is a tuple (for method4)\n",
    "        if configs.task_type == 'pretext':\n",
    "            if isinstance(outputs, tuple):\n",
    "                # Use the reconstructed image for visualization\n",
    "                outputs = outputs[0]\n",
    "\n",
    "        outputs = outputs.to(device)\n",
    "\n",
    "        # Calculate loss\n",
    "        if configs.task_type == 'pretext':\n",
    "            total_loss = loss_fn(outputs, imgs)\n",
    "        else:\n",
    "            grayscale_outputs = 0.299 * outputs[:, 0, :, :] + 0.587 * outputs[:, 1, :, :] + 0.114 * outputs[:, 2, :, :]\n",
    "            grayscale_outputs = grayscale_outputs.unsqueeze(1)  # shape: (batch_size, 1, H, W)\n",
    "            masks = masks.unsqueeze(1)\n",
    "            total_loss = loss_fn(grayscale_outputs, masks)\n",
    "            #total_loss = loss_fn(outputs, masks)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        if scaler:\n",
    "            scaler.scale(total_loss).backward()\n",
    "\n",
    "            # Unscale the gradients for gradient clipping\n",
    "            scaler.unscale_(optimizer)\n",
    "\n",
    "            # Compute gradient norm for monitoring\n",
    "            total_norm = 0\n",
    "            for p in model.parameters():\n",
    "                if p.grad is not None:\n",
    "                    param_norm = p.grad.data.norm(2)\n",
    "                    total_norm += param_norm.item() ** 2\n",
    "            total_norm = total_norm ** 0.5\n",
    "            grad_norm.update(total_norm)\n",
    "\n",
    "            # Gradient clipping\n",
    "            torch.nn.utils.clip_grad_norm_(\n",
    "                model.parameters(), configs.clip_grad_norm)\n",
    "\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "        else:\n",
    "            total_loss.backward()\n",
    "\n",
    "            # Compute gradient norm for monitoring\n",
    "            total_norm = 0\n",
    "            for p in model.parameters():\n",
    "                if p.grad is not None:\n",
    "                    param_norm = p.grad.data.norm(2)\n",
    "                    total_norm += param_norm.item() ** 2\n",
    "            total_norm = total_norm ** 0.5\n",
    "            grad_norm.update(total_norm)\n",
    "\n",
    "            # Gradient clipping\n",
    "            torch.nn.utils.clip_grad_norm_(\n",
    "                model.parameters(), configs.clip_grad_norm)\n",
    "\n",
    "            optimizer.step()\n",
    "\n",
    "        #if batch_idx % configs.subdivisions:\n",
    "        #    optimizer.zero_grad()\n",
    "\n",
    "        reduced_loss = total_loss.data\n",
    "\n",
    "        losses.update(to_python_float(reduced_loss), imgs.size(0))\n",
    "\n",
    "        # measure elapsed time\n",
    "        # torch.cuda.synchronize()\n",
    "        batch_time.update(time.time() - start_time)\n",
    "\n",
    "        if tb_writer is not None:\n",
    "            tb_writer.add_scalar('avg_loss', losses.avg, batch_idx)\n",
    "            tb_writer.add_scalar('Loss/train', losses.avg, epoch)\n",
    "            tb_writer.add_scalar('GradNorm', grad_norm.avg, batch_idx)\n",
    "\n",
    "        # Log message\n",
    "        if logger is not None and batch_idx % 10 == 0:\n",
    "            logger.info(progress.get_message(batch_idx))\n",
    "\n",
    "        # More detailed logging every 50 batches\n",
    "        if logger is not None and batch_idx % 50 == 0:\n",
    "            logger.info(\n",
    "                f\"Epoch {epoch+1} - Batch {batch_idx}/{len(train_dataloader)} - \"\n",
    "                f\"Loss: {total_loss:.4f}, Grad Norm: {grad_norm.val:.4f}, \"\n",
    "                f\"LR: {optimizer.param_groups[0]['lr']:.8f}\")\n",
    "\n",
    "        start_time = time.time()\n",
    "\n",
    "    # Visualize outputs every few epochs\n",
    "    #if epoch % 5 == 0 and configs.task_type == 'pretext':\n",
    "    #    visualize_reconstructions(\n",
    "    #        model, train_dataloader, device, epoch, configs.results_dir)\n",
    "    #elif configs.task_type == 'segmentation':\n",
    "    #    visualize_segmentation(\n",
    "    #        model, train_dataloader, device, epoch, configs.results_dir)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    try:\n",
    "        configs = get_configs()  # Using our previously defined get_configs()\n",
    "\n",
    "        # Create necessary directories\n",
    "        try:\n",
    "            os.makedirs(configs.checkpoints_dir, exist_ok=True)\n",
    "            os.makedirs(configs.logs_dir, exist_ok=True)\n",
    "            os.makedirs(configs.results_dir, exist_ok=True)\n",
    "            os.makedirs(configs.model_dir, exist_ok=True)\n",
    "        except Exception as e:\n",
    "            print(f\"Error creating directories: {e}\")\n",
    "            sys.exit(1)\n",
    "\n",
    "        # Set seeds for reproducibility\n",
    "        if configs.seed is not None:\n",
    "            random.seed(configs.seed)\n",
    "            np.random.seed(configs.seed)\n",
    "            torch.manual_seed(configs.seed)\n",
    "            torch.backends.cudnn.deterministic = True\n",
    "            torch.backends.cudnn.benchmark = False\n",
    "\n",
    "        # Initialize logging\n",
    "        try:\n",
    "            logger = Logger(configs.logs_dir, configs.saved_fn)\n",
    "            writer = SummaryWriter(log_dir=os.path.join(configs.logs_dir, 'tensorboard'))\n",
    "        except Exception as e:\n",
    "            print(f\"Error initializing logger or TensorBoard writer: {e}\")\n",
    "            sys.exit(1)\n",
    "\n",
    "        # Create model and loss function\n",
    "        try:\n",
    "            model, loss_fn = create_model(configs, logger)\n",
    "        except Exception as e:\n",
    "            logger.info(f\"Error creating model: {e}\")\n",
    "            sys.exit(1)\n",
    "\n",
    "        # Load pretrained weights if specified\n",
    "        if configs.pretrained_path is not None:\n",
    "            try:\n",
    "                checkpoint = torch.load(configs.pretrained_path, map_location=configs.device)\n",
    "                # Remove 'module.' from keys if present\n",
    "                new_state_dict = {}\n",
    "                for k, v in checkpoint.items():\n",
    "                    if k.startswith('module.'):\n",
    "                        k = k[7:]  # remove 'module.'\n",
    "                    new_state_dict[k] = v\n",
    "                # Load into your model\n",
    "                model.load_state_dict(new_state_dict)\n",
    "                #model.load_state_dict(torch.load(configs.pretrained_path))\n",
    "                logger.info(f'Loaded pretrained model from {configs.pretrained_path}')\n",
    "            except Exception as e:\n",
    "                logger.info(f\"Failed to load pretrained model from {configs.pretrained_path}. Error: {e}\")\n",
    "                sys.exit(1)\n",
    "\n",
    "\n",
    "        try:\n",
    "            model = make_data_parallel(model, configs)\n",
    "        except Exception as e:\n",
    "            logger.info(f\"Error setting up data parallelism: {e}\")\n",
    "            sys.exit(1)\n",
    "\n",
    "        # Move model to device and setup parallel processing\n",
    "        try:\n",
    "            device = torch.device(configs.device)\n",
    "            model = model.to(device)\n",
    "            summary_x = summary(model, input_size=(1, 3, 256, 256))  # (batch_size, channels, height, width)\n",
    "            logger.info(f\"Model moved to device: {device}\")\n",
    "            print(f\"\\n\\nModel summary: {summary_x}\\n\\n\")\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.info(f\"Error moving model to device: {e}\")\n",
    "            sys.exit(1)\n",
    "\n",
    "        # Create optimizer and scheduler\n",
    "        try:\n",
    "            optimizer = create_optimizer(configs, model)\n",
    "            lr_scheduler = create_lr_scheduler(optimizer, configs)\n",
    "        except Exception as e:\n",
    "            logger.info(f\"Error creating optimizer or learning rate scheduler: {e}\")\n",
    "            sys.exit(1)\n",
    "\n",
    "        # Load dataset\n",
    "        try:\n",
    "            dataset_loader = DatasetLoader(configs.image_size,\n",
    "                                           configs.root_dir,\n",
    "                                           configs.split_ratios,\n",
    "                                           configs.seed,\n",
    "                                           configs.shuffle_data,\n",
    "                                           configs.batch_size,\n",
    "                                           configs.num_workers,\n",
    "                                           configs.task_type,)\n",
    "            train_dataloader, val_dataloader, _ = dataset_loader.get_dataloaders()\n",
    "        except Exception as e:\n",
    "            logger.info(f\"Error loading dataset: {e}\")\n",
    "            sys.exit(1)\n",
    "\n",
    "        # Setup AMP if enabled\n",
    "        scaler = GradScaler() if configs.use_amp else None\n",
    "\n",
    "        # Training loop\n",
    "        best_val_loss = float(\"inf\")\n",
    "        val_losses = []\n",
    "\n",
    "        for epoch in range(configs.start_epoch, configs.num_epochs):\n",
    "            try:\n",
    "                # Clear GPU cache\n",
    "                if torch.cuda.is_available():\n",
    "                    torch.cuda.empty_cache()\n",
    "\n",
    "                # Train for one epoch\n",
    "                train_one_epoch(train_dataloader, model, optimizer,\n",
    "                                lr_scheduler, epoch, configs, logger, writer,\n",
    "                                device, scaler, loss_fn)\n",
    "\n",
    "                # Validation phase\n",
    "                model.eval()\n",
    "                running_val_loss = 0.0\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    for images, masks in tqdm(val_dataloader, desc=f\"Validation Epoch {epoch}\"):\n",
    "                        images = images.to(device)\n",
    "                        masks = masks.to(device)\n",
    "\n",
    "                        with autocast() if scaler else torch.no_grad():\n",
    "                            outputs = model(images)\n",
    "\n",
    "                            if configs.task_type == 'pretext':\n",
    "                                loss = loss_fn(outputs, images)\n",
    "                            else:\n",
    "                                grayscale_outputs = 0.299 * outputs[:, 0, :, :] + 0.587 * outputs[:, 1, :, :] + 0.114 * outputs[:, 2, :, :]\n",
    "                                grayscale_outputs = grayscale_outputs.unsqueeze(1)  # shape: (batch_size, 1, H, W)\n",
    "                                masks = masks.unsqueeze(1)\n",
    "                                \n",
    "                                loss = loss_fn(grayscale_outputs, masks)\n",
    "\n",
    "#                            loss = loss_fn(outputs, images if configs.task_type == \"pretext\" else masks)\n",
    "\n",
    "\n",
    "                        running_val_loss += loss.item()\n",
    "\n",
    "                avg_val_loss = running_val_loss / len(val_dataloader)\n",
    "                val_losses.append(avg_val_loss)\n",
    "                writer.add_scalar('Loss/val', avg_val_loss, epoch)\n",
    "\n",
    "                # Save best model\n",
    "                if epoch % configs.save_checkpoint_freq == 0:\n",
    "                    save_checkpoint(configs.checkpoints_dir,\n",
    "                                    configs.saved_fn + '_best',\n",
    "                                    model.state_dict(),\n",
    "                                    {'optimizer': optimizer.state_dict(),\n",
    "                                     'lr_scheduler': lr_scheduler.state_dict(),\n",
    "                                     'epoch': epoch},\n",
    "                                    epoch, model_type=configs.task_type)\n",
    "\n",
    "                # Early stopping check\n",
    "                if len(val_losses) > configs.patience:\n",
    "                    if all(val_losses[-i-1] >= val_losses[-i-2]\n",
    "                           for i in range(configs.patience)):\n",
    "                        print(f\"Early stopping triggered after {epoch + 1} epochs\")\n",
    "                        break\n",
    "\n",
    "                # Log validation loss\n",
    "                logger.info(\n",
    "                    f\"Validation Loss: {avg_val_loss:.4f} at epoch {epoch+1}/{configs.num_epochs}\")\n",
    "                \n",
    "\n",
    "                if epoch % 5 == 0:\n",
    "                    if configs.task_type == 'pretext':\n",
    "                        visualize_reconstructions(model, train_dataloader,\n",
    "                                                device, epoch, configs.results_dir)\n",
    "                    elif configs.task_type == 'segmentation':\n",
    "                        visualize_segmentation(model, train_dataloader,\n",
    "                                            device, epoch, configs.results_dir)\n",
    "\n",
    "            except Exception as e:\n",
    "                logger.info(f\"Error during training or validation at epoch {epoch}: {e}\")\n",
    "                cleanup()\n",
    "                sys.exit(1)\n",
    "\n",
    "        writer.close()\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"Training interrupted by user. Cleaning up...\")\n",
    "        cleanup()\n",
    "        sys.exit(0)\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred: {e}\")\n",
    "        cleanup()\n",
    "        sys.exit(1)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    try:\n",
    "        main()\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"Training interrupted by user. Cleaning up...\")\n",
    "        cleanup()\n",
    "        sys.exit(0)\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred: {e}\")\n",
    "        cleanup()\n",
    "        sys.exit(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def main():\n",
    "    \n",
    "    configs = get_configs()  # Using our previously defined get_configs()\n",
    "\n",
    "    # Create necessary directories\n",
    "    try:\n",
    "        os.makedirs(configs.logs_dir, exist_ok=True)\n",
    "        os.makedirs(configs.results_dir, exist_ok=True)\n",
    "    except Exception as e:\n",
    "        print(f\"Error creating directories: {e}\")\n",
    "        sys.exit(1)\n",
    "\n",
    "    # Initialize logging\n",
    "    try:\n",
    "        logger = Logger(configs.logs_dir, configs.saved_fn)\n",
    "    except Exception as e:\n",
    "        print(f\"Error initializing logger or TensorBoard writer: {e}\")\n",
    "        sys.exit(1)\n",
    "\n",
    "    # log the configurations\n",
    "    try:\n",
    "        logger.info(\"Configurations:\")\n",
    "        for key, value in vars(configs).items():\n",
    "            logger.info(f\"{key}: {value}\")\n",
    "    except Exception as e:\n",
    "        logger.info(f\"Error logging configurations: {e}\")\n",
    "        sys.exit(1)\n",
    "\n",
    "    # Create model and loss function\n",
    "    try:\n",
    "        model, loss_fn = create_model(configs, logger)\n",
    "    except Exception as e:\n",
    "        logger.info(f\"Error creating model: {e}\")\n",
    "        sys.exit(1)\n",
    "\n",
    "    # Load pretrained weights if specified\n",
    "    if configs.pretrained_path is not None:\n",
    "        try:\n",
    "            checkpoint = torch.load(configs.pretrained_path, map_location=configs.device)\n",
    "            # Remove 'module.' from keys if present\n",
    "            new_state_dict = {}\n",
    "            for k, v in checkpoint.items():\n",
    "                if k.startswith('module.'):\n",
    "                    k = k[7:]  # remove 'module.'\n",
    "                new_state_dict[k] = v\n",
    "            # Load into your model\n",
    "            model.load_state_dict(new_state_dict)\n",
    "            #model.load_state_dict(torch.load(configs.pretrained_path))\n",
    "            logger.info(f'Loaded pretrained model from {configs.pretrained_path}')\n",
    "        except Exception as e:\n",
    "            logger.info(f\"Failed to load pretrained model from {configs.pretrained_path}. Error: {e}\")\n",
    "            sys.exit(1)\n",
    "\n",
    "\n",
    "    try:\n",
    "        model = make_data_parallel(model, configs)\n",
    "    except Exception as e:\n",
    "        logger.info(f\"Error setting up data parallelism: {e}\")\n",
    "        sys.exit(1)\n",
    "\n",
    "    # Move model to device and setup parallel processing\n",
    "    try:\n",
    "        device = torch.device(configs.device)\n",
    "        model = model.to(device)\n",
    "        summary_x = summary(model, input_size=(1, 3, 256, 256))  # (batch_size, channels, height, width)\n",
    "        logger.info(f\"Model moved to device: {device}\")\n",
    "        print(f\"\\n\\nModel summary: {summary_x}\\n\\n\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.info(f\"Error moving model to device: {e}\")\n",
    "        sys.exit(1)\n",
    "\n",
    "    # Load dataset\n",
    "    try:\n",
    "        dataset_loader = DatasetLoader(configs.image_size,\n",
    "                                        configs.root_dir,\n",
    "                                        configs.split_ratios,\n",
    "                                        configs.seed,\n",
    "                                        configs.shuffle_data,\n",
    "                                        configs.batch_size,\n",
    "                                        configs.num_workers,\n",
    "                                        configs.task_type)\n",
    "        _, _, test_dataloader = dataset_loader.get_dataloaders()\n",
    "    except Exception as e:\n",
    "        logger.info(f\"Error loading dataset: {e}\")\n",
    "        sys.exit(1)\n",
    "\n",
    "    # Setup AMP if enabled\n",
    "    scaler = GradScaler() if configs.use_amp else None\n",
    "\n",
    "\n",
    "\n",
    "    try:\n",
    "        # Clear GPU cache\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "        # Evaluation phase\n",
    "        model.eval()\n",
    "        \n",
    "        # Initialize metrics storage\n",
    "        all_metrics = []\n",
    "        running_val_loss = 0.0\n",
    "        total_samples = 0\n",
    "        batch = 0\n",
    "        \n",
    "        # Lists to store all predictions and ground truth for overall metrics\n",
    "        all_predictions = []\n",
    "        all_masks = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for images, masks in tqdm(test_dataloader, desc=\"Testing\", unit=\"batch\"):\n",
    "                # Move data to device\n",
    "                images = images.to(device)\n",
    "                masks = masks.to(device)\n",
    "                \n",
    "                # Ensure masks have the correct shape\n",
    "                if masks.ndim == 2:\n",
    "                    masks = masks.unsqueeze(0)  # Add batch dimension\n",
    "                if masks.ndim == 3:\n",
    "                    masks = masks.unsqueeze(1)  # Add channel dimension\n",
    "                \n",
    "                \n",
    "                # Forward pass\n",
    "                outputs = model(images)\n",
    "                \n",
    "                # Store predictions and masks for overall metrics\n",
    "                all_predictions.append(outputs.cpu())\n",
    "                all_masks.append(masks.cpu())\n",
    "                \n",
    "                if configs.task_type == 'pretext':\n",
    "                    visualize_reconstructions_2(outputs, images, configs.results_dir, batch)\n",
    "                    loss = loss_fn(outputs, images)\n",
    "                elif configs.task_type == 'segmentation':\n",
    "                    visualize_segmentation_2(outputs, images, masks, batch, configs.results_dir)\n",
    "                    \n",
    "                    # Calculate metrics for current batch\n",
    "                    batch_metrics = calculate_metrics(outputs, masks)\n",
    "                    all_metrics.append(batch_metrics)\n",
    "                    \n",
    "                    # Calculate loss\n",
    "                    grayscale_outputs = 0.299 * outputs[:, 0, :, :] + 0.587 * outputs[:, 1, :, :] + 0.114 * outputs[:, 2, :, :]\n",
    "                    grayscale_outputs = grayscale_outputs.unsqueeze(1)\n",
    "                    loss = loss_fn(grayscale_outputs, masks)\n",
    "                \n",
    "                running_val_loss += loss.item() * images.size(0)\n",
    "                total_samples += images.size(0)\n",
    "                batch += 1\n",
    "\n",
    "        # Calculate average loss\n",
    "        avg_val_loss = running_val_loss / total_samples\n",
    "        \n",
    "        # Calculate overall metrics\n",
    "        if configs.task_type == 'segmentation':\n",
    "            # Concatenate all predictions and masks\n",
    "            all_predictions = torch.cat(all_predictions, dim=0)\n",
    "            all_masks = torch.cat(all_masks, dim=0)\n",
    "            \n",
    "            # Calculate overall metrics\n",
    "            overall_metrics = calculate_metrics(all_predictions, all_masks)\n",
    "            \n",
    "            # Calculate mean metrics across all batches\n",
    "            mean_metrics = {\n",
    "                'iou': np.mean([m['iou'] for m in all_metrics]),\n",
    "                'dice': np.mean([m['dice'] for m in all_metrics]),\n",
    "                'precision': np.mean([m['precision'] for m in all_metrics]),\n",
    "                'recall': np.mean([m['recall'] for m in all_metrics]),\n",
    "                'f1': np.mean([m['f1'] for m in all_metrics])\n",
    "            }\n",
    "            \n",
    "            # Log results\n",
    "            logger.info(f\"\\nTest Results:\")\n",
    "            logger.info(f\"Average Loss: {avg_val_loss:.4f}\")\n",
    "            logger.info(\"\\nBatch-wise Mean Metrics:\")\n",
    "            for metric, value in mean_metrics.items():\n",
    "                logger.info(f\"{metric.upper()}: {value:.4f}\")\n",
    "            \n",
    "            logger.info(\"\\nOverall Metrics:\")\n",
    "            for metric, value in overall_metrics.items():\n",
    "                logger.info(f\"{metric.upper()}: {value:.4f}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred: {e}\")\n",
    "        cleanup()\n",
    "        sys.exit(1)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    try:\n",
    "        main()\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"Training interrupted by user. Cleaning up...\")\n",
    "        cleanup()\n",
    "        sys.exit(0)\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred: {e}\")\n",
    "        cleanup()\n",
    "        sys.exit(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "doc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
